{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481a4d5d-dbe6-479c-b26b-343102d0617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function implementation below is a modification version from Tensorflow\n",
    "# Original code link: https://github.com/ashesh6810/DDWP-DA/blob/master/Unet_STN_lead12.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aaa806e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T22:07:38.028628Z",
     "start_time": "2023-08-30T22:07:38.009616Z"
    }
   },
   "outputs": [],
   "source": [
    "# Basic setting for Jupyter_notebook to import utils\n",
    "import os\n",
    "import sys\n",
    "\n",
    "notebook_path = os.path.abspath(\"\")\n",
    "project_root = os.path.abspath(os.path.join(notebook_path, \"../../\"))\n",
    "\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36e8b9c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T22:07:38.452810Z",
     "start_time": "2023-08-30T22:07:38.436244Z"
    }
   },
   "outputs": [],
   "source": [
    "import netCDF4 as nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T22:07:39.573064Z",
     "start_time": "2023-08-30T22:07:38.793329Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import scipy.io as sio\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19831d32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T22:07:39.667111Z",
     "start_time": "2023-08-30T22:07:39.653067Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from utils import folder_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdca99b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T22:07:39.791115Z",
     "start_time": "2023-08-30T22:07:39.783113Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    "country = \"GB\"\n",
    "data_folder = \"data\"\n",
    "data_test_category = \"test_data\"\n",
    "data_read_category = \"raw_data\"\n",
    "data_save_category = \"processed_data\"\n",
    "output_folder = \"ERA5_DATA\"\n",
    "ddeg_out_lat = 0.25\n",
    "ddeg_out_lon = 0.125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f685f847",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T22:07:41.529674Z",
     "start_time": "2023-08-30T22:07:40.809932Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import keras.backend as K\n",
    "# from tensorflow.contrib import keras\n",
    "# from tensorflow.contrib.keras import backend as K\n",
    "\n",
    "#from data_manager import ClutteredMNIST\n",
    "#from visualizer import plot_mnist_sample\n",
    "#from visualizer import print_evaluation\n",
    "#from visualizer import plot_mnist_grid\n",
    "# import netCDF4 as nc\n",
    "\n",
    "from keras.layers import Input, Convolution2D, Convolution1D, MaxPooling2D, Dense, Dropout, \\\n",
    "                          Flatten, concatenate, Activation, Reshape, \\\n",
    "                          UpSampling2D,ZeroPadding2D\n",
    "import keras\n",
    "from keras.callbacks import History\n",
    "history = History()\n",
    "\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Cropping2D, Concatenate, ZeroPadding2D\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Dense\n",
    "\n",
    "from utils import model_utils_tf\n",
    "from bilinear_interpolation_1x import BilinearInterpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cab469b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T22:07:44.857929Z",
     "start_time": "2023-08-30T22:07:44.842025Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_initial_weights(output_size):\n",
    "    b = np.zeros((2, 3), dtype='float32')\n",
    "    b[0, 0] = 1\n",
    "    b[1, 1] = 1\n",
    "    W = np.zeros((output_size, 6), dtype='float32')\n",
    "    weights = [W, b.flatten()]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbdabaa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T22:07:45.455256Z",
     "start_time": "2023-08-30T22:07:45.439253Z"
    }
   },
   "outputs": [],
   "source": [
    "losses = []  # 用于存储每个 epoch 的训练损失\n",
    "val_losses = []  # 用于存储每个 epoch 的验证损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37500c3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T22:07:46.019273Z",
     "start_time": "2023-08-30T22:07:45.989665Z"
    }
   },
   "outputs": [],
   "source": [
    "def stn(input_shape=(32, 64, 1), sampling_size=(8, 16), num_classes=10):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    conv1 = Convolution2D(32, 5, 5, activation='relu', border_mode='same')(inputs)\n",
    "    conv1 = Convolution2D(32, 5, 5, activation='relu', border_mode='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Convolution2D(32, 5, 5, activation='relu', border_mode='same')(pool1)\n",
    "    conv2 = Convolution2D(32, 5, 5, activation='relu', border_mode='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Convolution2D(32, 5, 5, activation='relu', border_mode='same')(pool2)\n",
    "#     conv3 = Convolution2D(32, 5, 5, activation='relu', border_mode='same')(conv3)\n",
    "\n",
    "\n",
    "    conv5 = Convolution2D(32, 5, 5, activation='relu', border_mode='same')(conv3)\n",
    "#     conv5 = Convolution2D(32, 5, 5, activation='relu', border_mode='same')(conv5)\n",
    "    \n",
    "    locnet = Flatten()(conv5)\n",
    "    locnet = Dense(500)(locnet)\n",
    "    locnet = Activation('relu')(locnet)\n",
    "    locnet = Dense(200)(locnet)\n",
    "    locnet = Activation('relu')(locnet)\n",
    "    locnet = Dense(100)(locnet)\n",
    "    locnet = Activation('relu')(locnet)\n",
    "    locnet = Dense(50)(locnet)\n",
    "    locnet = Activation('relu')(locnet)\n",
    "    weights = get_initial_weights(50)\n",
    "    locnet = Dense(6, weights=weights)(locnet)\n",
    "    x = BilinearInterpolation(sampling_size)([inputs, locnet])\n",
    "\n",
    "\n",
    "    up6 = keras.layers.Concatenate(axis=-1)([Convolution2D(32, 2, 2,activation='relu', border_mode='same')(UpSampling2D(size=(2, 2))(x)), conv2])\n",
    "    conv6 = Convolution2D(32, 5, 5, activation='relu', border_mode='same')(up6)\n",
    "    conv6 = Convolution2D(32, 5, 5, activation='relu', border_mode='same')(conv6)\n",
    "\n",
    "    up7 = keras.layers.Concatenate(axis=-1)([Convolution2D(32, 2, 2,activation='relu', border_mode='same')(UpSampling2D(size=(2, 2))(conv6)), conv1])\n",
    "    conv7 = Convolution2D(32, 5, 5, activation='relu', border_mode='same')(up7)\n",
    "    conv7 = Convolution2D(32, 5, 5, activation='relu', border_mode='same')(conv7)\n",
    "\n",
    "\n",
    "\n",
    "    conv10 = Convolution2D(1, 5, 5, activation='linear',border_mode='same')(conv7)\n",
    "\n",
    "    model = Model(input=inputs, output=conv10)\n",
    "\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b3a34bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T22:07:46.853595Z",
     "start_time": "2023-08-30T22:07:46.586090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "  after removing the cwd from sys.path.\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "  import sys\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "  \n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:32: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (2, 2), activation=\"relu\", padding=\"same\")`\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 64, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 64, 32)   832         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 64, 32)   25632       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 32, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 32, 32)   25632       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 32, 32)   25632       conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 8, 16, 32)    0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 8, 16, 32)    25632       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 8, 16, 32)    25632       conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4096)         0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 500)          2048500     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 500)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 200)          100200      activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 200)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 100)          20100       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 100)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 50)           5050        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 50)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 6)            306         activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bilinear_interpolation_1 (Bilin (None, 8, 16, 1)     0           input_1[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 16, 32, 1)    0           bilinear_interpolation_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 32, 32)   160         up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 16, 32, 64)   0           conv2d_7[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 32, 32)   51232       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 32, 32)   25632       conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 32, 64, 32)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 64, 32)   4128        up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 64, 64)   0           conv2d_10[0][0]                  \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 64, 32)   51232       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 64, 32)   25632       conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 64, 1)    801         conv2d_12[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,461,965\n",
      "Trainable params: 2,461,965\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:36: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (2, 2), activation=\"relu\", padding=\"same\")`\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:37: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (5, 5), activation=\"linear\", padding=\"same\")`\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:44: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"
     ]
    }
   ],
   "source": [
    "model = stn()\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a9ec3e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T22:07:47.651035Z",
     "start_time": "2023-08-30T22:07:47.645052Z"
    }
   },
   "outputs": [],
   "source": [
    "fileList_train = []\n",
    "fileList_validation =[]\n",
    "fileList_test=[]\n",
    "input_folder_path = folder_utils.find_folder(\n",
    "    country, data_folder, data_save_category, output_folder\n",
    ")\n",
    "\n",
    "for year in range (1979,2021):\n",
    "    file_path =  os.path.join(input_folder_path, f\"era5_pressure_level_{year}_regrid_filter_850.nc\")\n",
    "    fileList_train.append (file_path)\n",
    "    \n",
    "fileList_validation.append(os.path.join(input_folder_path, f\"era5_pressure_level_2021_regrid_filter_850.nc\"))\n",
    "fileList_test.append(os.path.join(input_folder_path, f\"era5_pressure_level_2022_regrid_filter_850.nc\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d771a97a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T22:07:48.123279Z",
     "start_time": "2023-08-30T22:07:48.114967Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "539865a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T22:07:48.783713Z",
     "start_time": "2023-08-30T22:07:48.763778Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_1979_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_1980_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_1981_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_1982_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_1983_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_1984_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_1985_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_1986_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_1987_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_1988_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_1989_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_1990_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_1991_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_1992_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_1993_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_1994_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_1995_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_1996_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_1997_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_1998_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_1999_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2000_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2001_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2002_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2003_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2004_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2005_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2006_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2007_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2008_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2009_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2010_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2011_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2012_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2013_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2014_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2015_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2016_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2017_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2018_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2019_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2020_regrid_filter_850.nc']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileList_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c4bdb87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T22:07:49.564000Z",
     "start_time": "2023-08-30T22:07:49.555002Z"
    }
   },
   "outputs": [],
   "source": [
    "M = 273.77817\n",
    "sdev = 2.5819736"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7acf4d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T22:07:50.487463Z",
     "start_time": "2023-08-30T22:07:50.185635Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bf864b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T22:07:50.596923Z",
     "start_time": "2023-08-30T22:07:50.587920Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'netCDF4._netCDF4.Dataset'>\n",
       "root group (NETCDF3_64BIT_OFFSET data model, file format NETCDF3):\n",
       "    regrid_method: bilinear\n",
       "    Conventions: CF-1.6\n",
       "    history: 2023-08-26 11:48:45 GMT by grib_to_netcdf-2.25.1: /opt/ecmwf/mars-client/bin/grib_to_netcdf.bin -S param -o /cache/data6/adaptor.mars.internal-1693050523.9571652-30807-11-9f2e4cee-f5dd-4cb7-bd14-d8cfa9322780.nc /cache/tmp/9f2e4cee-f5dd-4cb7-bd14-d8cfa9322780-adaptor.mars.internal-1693050488.2242575-30807-18-tmp.grib\n",
       "    dimensions(sizes): time(8760), lat(32), lon(64)\n",
       "    variables(dimensions): float32 t(time, lat, lon), int32 time(time), float64 lon(lon), float64 lat(lat)\n",
       "    groups: "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "File=nc.Dataset(fileList_train[0])\n",
    "File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f269b6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T22:07:55.822062Z",
     "start_time": "2023-08-30T22:07:55.743264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8760, 32, 64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z=np.asarray(File['t'])\n",
    "# Filter out zero and NaN values\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38b68814",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T22:07:56.246838Z",
     "start_time": "2023-08-30T22:07:56.237836Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[261.73975, 261.64294, 261.54614, ..., 260.0619 , 260.09912,\n",
       "        260.09912],\n",
       "       [261.7046 , 261.5629 , 261.42117, ..., 259.8187 , 259.87036,\n",
       "        259.87036],\n",
       "       [261.7162 , 261.5668 , 261.41742, ..., 259.6002 , 259.62454,\n",
       "        259.62454],\n",
       "       ...,\n",
       "       [261.46216, 261.42407, 261.38602, ..., 256.72403, 256.6516 ,\n",
       "        256.6516 ],\n",
       "       [261.79437, 261.66544, 261.5366 , ..., 256.6801 , 256.59903,\n",
       "        256.59903],\n",
       "       [262.21606, 262.0862 , 261.95657, ..., 256.4985 , 256.44095,\n",
       "        256.44095]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2df9f97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T21:57:39.475339Z",
     "start_time": "2023-08-30T21:57:39.460295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([270.38263, 270.43054, 270.47824, 270.53998, 270.6015 , 270.64752,\n",
       "       270.69333, 270.71683, 270.7401 , 270.7686 , 270.79678, 270.85736,\n",
       "       270.91766, 270.99496, 271.072  , 271.1141 , 271.156  , 271.10828,\n",
       "       271.0604 , 270.9365 , 270.8125 , 270.69632, 270.58   , 270.54492,\n",
       "       270.50964, 270.5439 , 270.57794, 270.60828, 270.63837, 270.62006,\n",
       "       270.6015 , 270.54102, 270.4803 , 270.39453, 270.30853, 270.19342,\n",
       "       270.07806, 269.91208, 269.74585, 269.51465, 269.2832 , 268.9912 ,\n",
       "       268.69897, 268.4043 , 268.1093 , 267.90137, 267.6931 , 267.61523,\n",
       "       267.53708, 267.53424, 267.53125, 267.5127 , 267.49405, 267.43845,\n",
       "       267.38272, 267.3447 , 267.30658, 267.33893, 267.37112, 267.48727,\n",
       "       267.6033 , 267.79468, 267.98605, 267.98605], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z[500][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e449519b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c37012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88d1c98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660e912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13154bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a79e1b3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T21:04:14.961563Z",
     "start_time": "2023-08-30T21:04:14.949458Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8460"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainN=np.size(Z,0)-300\n",
    "trainN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b40356d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T16:48:59.034949Z",
     "start_time": "2023-08-30T16:48:59.014776Z"
    }
   },
   "outputs": [],
   "source": [
    "# mask = np.all(~np.isnan(Z), axis=(1, 2))\n",
    "# Z = Z[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "aa613ea5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T20:12:16.745211Z",
     "start_time": "2023-08-30T20:12:16.738180Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[261.73975, 261.64294, 261.54614, ..., 260.0619 , 260.09912,\n",
       "         260.09912],\n",
       "        [261.7046 , 261.5629 , 261.42117, ..., 259.8187 , 259.87036,\n",
       "         259.87036],\n",
       "        [261.7162 , 261.5668 , 261.41742, ..., 259.6002 , 259.62454,\n",
       "         259.62454],\n",
       "        ...,\n",
       "        [261.46216, 261.42407, 261.38602, ..., 256.72403, 256.6516 ,\n",
       "         256.6516 ],\n",
       "        [261.79437, 261.66544, 261.5366 , ..., 256.6801 , 256.59903,\n",
       "         256.59903],\n",
       "        [262.21606, 262.0862 , 261.95657, ..., 256.4985 , 256.44095,\n",
       "         256.44095]],\n",
       "\n",
       "       [[261.59326, 261.47308, 261.3529 , ..., 259.57553, 259.56784,\n",
       "         259.56784],\n",
       "        [261.50928, 261.3765 , 261.24365, ..., 259.3987 , 259.39575,\n",
       "         259.39575],\n",
       "        [261.43893, 261.31006, 261.18115, ..., 259.36264, 259.32166,\n",
       "         259.32166],\n",
       "        ...,\n",
       "        [261.84695, 261.7033 , 261.5598 , ..., 256.76602, 256.68506,\n",
       "         256.68506],\n",
       "        [262.0327 , 261.89386, 261.75513, ..., 256.7269 , 256.62054,\n",
       "         256.62054],\n",
       "        [262.35672, 262.2327 , 262.10886, ..., 256.67517, 256.5775 ,\n",
       "         256.5775 ]],\n",
       "\n",
       "       [[261.50278, 261.365  , 261.22723, ..., 259.26944, 259.2353 ,\n",
       "         259.2353 ],\n",
       "        [261.44235, 261.29196, 261.14154, ..., 259.14554, 259.11032,\n",
       "         259.11032],\n",
       "        [261.38364, 261.23428, 261.08487, ..., 259.124  , 259.0557 ,\n",
       "         259.0557 ],\n",
       "        ...,\n",
       "        [261.9931 , 261.86807, 261.74316, ..., 256.8437 , 256.77042,\n",
       "         256.77042],\n",
       "        [262.61624, 262.4276 , 262.2393 , ..., 256.76053, 256.69803,\n",
       "         256.69803],\n",
       "        [263.02426, 262.83276, 262.64148, ..., 256.60043, 256.5789 ,\n",
       "         256.5789 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[268.54227, 268.4359 , 268.32956, ..., 265.01605, 265.06778,\n",
       "         265.06778],\n",
       "        [268.2162 , 268.17905, 268.14175, ..., 265.25226, 265.3119 ,\n",
       "         265.3119 ],\n",
       "        [268.01883, 268.018  , 268.01712, ..., 265.65732, 265.66904,\n",
       "         265.66904],\n",
       "        ...,\n",
       "        [263.77304, 263.73193, 263.69073, ..., 264.83633, 264.84106,\n",
       "         264.84106],\n",
       "        [263.61084, 263.57758, 263.54425, ..., 264.60892, 264.62048,\n",
       "         264.62048],\n",
       "        [263.4975 , 263.46524, 263.43295, ..., 264.35507, 264.39615,\n",
       "         264.39615]],\n",
       "\n",
       "       [[268.4798 , 268.4308 , 268.38177, ..., 265.81723, 265.81143,\n",
       "         265.81143],\n",
       "        [268.39954, 268.37805, 268.35654, ..., 266.04202, 266.0303 ,\n",
       "         266.0303 ],\n",
       "        [268.43848, 268.44818, 268.45792, ..., 266.24228, 266.2297 ,\n",
       "         266.2297 ],\n",
       "        ...,\n",
       "        [263.7802 , 263.72562, 263.67093, ..., 264.52936, 264.55594,\n",
       "         264.55594],\n",
       "        [263.70575, 263.63254, 263.5593 , ..., 264.38477, 264.41904,\n",
       "         264.41904],\n",
       "        [263.67264, 263.61597, 263.5593 , ..., 264.31747, 264.3801 ,\n",
       "         264.3801 ]],\n",
       "\n",
       "       [[268.59793, 268.5503 , 268.50266, ..., 266.00024, 266.00024,\n",
       "         266.00024],\n",
       "        [268.46133, 268.43594, 268.41046, ..., 266.12927, 266.10983,\n",
       "         266.10983],\n",
       "        [268.44565, 268.42123, 268.39682, ..., 266.19876, 266.18222,\n",
       "         266.18222],\n",
       "        ...,\n",
       "        [263.7761 , 263.74866, 263.72113, ..., 264.30725, 264.3306 ,\n",
       "         264.3306 ],\n",
       "        [263.6624 , 263.64212, 263.62177, ..., 264.2386 , 264.26605,\n",
       "         264.26605],\n",
       "        [263.60812, 263.58542, 263.5627 , ..., 264.20752, 264.2408 ,\n",
       "         264.2408 ]]], dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7490b7e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T21:04:20.174976Z",
     "start_time": "2023-08-30T21:04:20.128517Z"
    }
   },
   "outputs": [],
   "source": [
    "Z=(Z-M)/sdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "850ba1f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T21:04:21.583179Z",
     "start_time": "2023-08-30T21:04:21.566010Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-4.6624885, -4.69998  , -4.737471 , ..., -5.312323 ,\n",
       "         -5.2979035, -5.2979035],\n",
       "        [-4.6761045, -4.7309823, -4.785872 , ..., -5.4065127,\n",
       "         -5.3865023, -5.3865023],\n",
       "        [-4.671613 , -4.7294693, -4.787326 , ..., -5.49114  ,\n",
       "         -5.481708 , -5.481708 ],\n",
       "        ...,\n",
       "        [-4.7699986, -4.784749 , -4.799488 , ..., -6.6050787,\n",
       "         -6.6331263, -6.6331263],\n",
       "        [-4.6413317, -4.691269 , -4.7411704, ..., -6.622087 ,\n",
       "         -6.653491 , -6.653491 ],\n",
       "        [-4.47801  , -4.5283017, -4.5785108, ..., -6.6924243,\n",
       "         -6.714716 , -6.714716 ]],\n",
       "\n",
       "       [[-4.719222 , -4.765767 , -4.812312 , ..., -5.5006905,\n",
       "         -5.503669 , -5.503669 ],\n",
       "        [-4.751749 , -4.8031754, -4.8546257, ..., -5.5691724,\n",
       "         -5.5703187, -5.5703187],\n",
       "        [-4.778993 , -4.8289065, -4.878832 , ..., -5.5831428,\n",
       "         -5.5990167, -5.5990167],\n",
       "        ...,\n",
       "        [-4.6209664, -4.676601 , -4.732176 , ..., -6.5888147,\n",
       "         -6.620172 , -6.620172 ],\n",
       "        [-4.5490212, -4.6028   , -4.6565313, ..., -6.6039677,\n",
       "         -6.6451583, -6.6451583],\n",
       "        [-4.423534 , -4.471568 , -4.5195317, ..., -6.6240015,\n",
       "         -6.6618237, -6.6618237]],\n",
       "\n",
       "       [[-4.7542667, -4.8076315, -4.860985 , ..., -5.61924  ,\n",
       "         -5.6324654, -5.6324654],\n",
       "        [-4.777669 , -4.8359156, -4.8941736, ..., -5.667227 ,\n",
       "         -5.6808662, -5.6808662],\n",
       "        [-4.80041  , -4.8582544, -4.9161224, ..., -5.6755714,\n",
       "         -5.702023 , -5.702023 ],\n",
       "        ...,\n",
       "        [-4.564363 , -4.6127872, -4.6611648, ..., -6.5587344,\n",
       "         -6.587113 , -6.587113 ],\n",
       "        [-4.323021 , -4.396077 , -4.469015 , ..., -6.5909424,\n",
       "         -6.6151485, -6.6151485],\n",
       "        [-4.1649947, -4.239162 , -4.3132463, ..., -6.6529474,\n",
       "         -6.661292 , -6.661292 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-2.0278678, -2.0690587, -2.1102495, ..., -3.3935728,\n",
       "         -3.373539 , -3.373539 ],\n",
       "        [-2.1541588, -2.168543 , -2.1829865, ..., -3.3020902,\n",
       "         -3.2789948, -3.2789948],\n",
       "        [-2.2305954, -2.2309146, -2.2312572, ..., -3.1452103,\n",
       "         -3.1406715, -3.1406715],\n",
       "        ...,\n",
       "        [-3.8749921, -3.8909128, -3.9068692, ..., -3.463178 ,\n",
       "         -3.4613457, -3.4613457],\n",
       "        [-3.9378126, -3.9506958, -3.9636025, ..., -3.5512567,\n",
       "         -3.546777 , -3.546777 ],\n",
       "        [-3.9817102, -3.9942033, -4.006708 , ..., -3.649571 ,\n",
       "         -3.633662 , -3.633662 ]],\n",
       "\n",
       "       [[-2.0520623, -2.0710442, -2.0900264, ..., -3.0832763,\n",
       "         -3.085522 , -3.085522 ],\n",
       "        [-2.0831475, -2.0914683, -2.099801 , ..., -2.996214 ,\n",
       "         -3.0007527, -3.0007527],\n",
       "        [-2.068066 , -2.0643072, -2.0605369, ..., -2.9186544,\n",
       "         -2.9235241, -2.9235241],\n",
       "        ...,\n",
       "        [-3.8722146, -3.8933594, -3.91454  , ..., -3.5820699,\n",
       "         -3.5717752, -3.5717752],\n",
       "        [-3.901054 , -3.929409 , -3.9577756, ..., -3.6380706,\n",
       "         -3.6247973, -3.6247973],\n",
       "        [-3.9138782, -3.9358268, -3.9577756, ..., -3.6641326,\n",
       "         -3.639879 , -3.639879 ]],\n",
       "\n",
       "       [[-2.006309 , -2.0247593, -2.0432093, ..., -3.0123947,\n",
       "         -3.0123947, -3.0123947],\n",
       "        [-2.059213 , -2.0690467, -2.078916 , ..., -2.962422 ,\n",
       "         -2.969951 , -2.969951 ],\n",
       "        [-2.0652883, -2.0747437, -2.0841994, ..., -2.935509 ,\n",
       "         -2.9419153, -2.9419153],\n",
       "        ...,\n",
       "        [-3.87381  , -3.884436 , -3.895097 , ..., -3.668092 ,\n",
       "         -3.6590502, -3.6590502],\n",
       "        [-3.9178376, -3.9256976, -3.933581 , ..., -3.694686 ,\n",
       "         -3.6840484, -3.6840484],\n",
       "        [-3.9388645, -3.9476583, -3.956452 , ..., -3.7067182,\n",
       "         -3.693823 , -3.693823 ]]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51a1a424",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T22:08:04.253301Z",
     "start_time": "2023-08-30T22:08:04.240855Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_epochs = 8\n",
    "lead=12\n",
    "count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81c257ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T22:08:04.822288Z",
     "start_time": "2023-08-30T22:08:04.625768Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26088\\3163184545.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrainN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlead\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrainN\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlead\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainN' is not defined"
     ]
    }
   ],
   "source": [
    "x_train=Z[0:trainN,:,:]\n",
    "x_train=x_train.reshape([np.size(x_train,0),32,64,1])\n",
    "y_train=Z[lead:trainN+lead,:,:]\n",
    "y_train=y_train.reshape([np.size(y_train,0),32,64,1])\n",
    "\n",
    "x_val= Z[trainN+lead:np.size(Z,0)-lead,:,:]\n",
    "x_val=x_val.reshape([np.size(x_val,0),32,64,1])\n",
    "\n",
    "y_val= Z[trainN+lead*2:np.size(Z,0),:,:]\n",
    "y_val=y_val.reshape([np.size(y_val,0),32,64,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72399c53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T21:04:29.110055Z",
     "start_time": "2023-08-30T21:04:29.085054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan!!!\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values\n",
    "if np.isnan(x_train).any() or np.isnan(y_train).any() or np.isnan(x_val).any() or np.isnan(y_val).any():\n",
    "    print(\"Found NaN values in data, skipping this file:\")\n",
    "\n",
    "else:\n",
    "    print(\"nan!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b62a1a55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T20:34:46.117230Z",
     "start_time": "2023-08-30T20:34:46.103231Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.923532 , -5.909916 , -5.896288 , -5.864884 , -5.8334675,\n",
       "       -5.8014016, -5.7693353, -5.7389827, -5.708642 , -5.6810675,\n",
       "       -5.6534925, -5.6470747, -5.6406684, -5.6531615, -5.6656547,\n",
       "       -5.6648626, -5.664071 , -5.634912 , -5.6057534, -5.5641727,\n",
       "       -5.52258  , -5.4768977, -5.4312034, -5.3766446, -5.322098 ,\n",
       "       -5.266156 , -5.2102265, -5.165596 , -5.120966 , -5.0831437,\n",
       "       -5.0453215, -5.0070972, -4.9688845, -4.939395 , -4.9099054,\n",
       "       -4.8974123, -4.884919 , -4.8814797, -4.8780403, -4.879234 ,\n",
       "       -4.880416 , -4.886432 , -4.892448 , -4.9000597, -4.90766  ,\n",
       "       -4.913345 , -4.91903  , -4.928486 , -4.937941 , -4.9503756,\n",
       "       -4.9628096, -4.9617577, -4.960694 , -4.940979 , -4.9212756,\n",
       "       -4.8978143, -4.8743405, -4.8531837, -4.832015 , -4.805965 ,\n",
       "       -4.779915 , -4.746596 , -4.713265 , -4.713265 ], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z[6609][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf56349f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T21:04:38.212332Z",
     "start_time": "2023-08-30T21:04:38.195338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row Index: 6610\n",
      "Column Index: 18\n",
      "Channel Index: 0\n",
      "-2.4731073\n",
      "-----\n",
      "Row Index: 6610\n",
      "Column Index: 19\n",
      "Channel Index: 0\n",
      "-6.774948\n",
      "-----\n",
      "Row Index: 6610\n",
      "Column Index: 19\n",
      "Channel Index: 1\n",
      "-6.774948\n",
      "-----\n",
      "Row Index: 6780\n",
      "Column Index: 7\n",
      "Channel Index: 0\n",
      "-2.4583921\n",
      "-----\n",
      "Row Index: 6780\n",
      "Column Index: 8\n",
      "Channel Index: 0\n",
      "-6.7855973\n",
      "-----\n",
      "Row Index: 6780\n",
      "Column Index: 8\n",
      "Channel Index: 1\n",
      "-6.7855973\n",
      "-----\n",
      "Row Index: 6780\n",
      "Column Index: 8\n",
      "Channel Index: 2\n",
      "-6.7855973\n",
      "-----\n",
      "Row Index: 6780\n",
      "Column Index: 8\n",
      "Channel Index: 3\n",
      "-6.7855973\n",
      "-----\n",
      "Row Index: 6827\n",
      "Column Index: 13\n",
      "Channel Index: 0\n",
      "-6.7793093\n",
      "-----\n",
      "Row Index: 6827\n",
      "Column Index: 14\n",
      "Channel Index: 0\n",
      "-2.4690888\n",
      "-----\n",
      "Row Index: 6827\n",
      "Column Index: 14\n",
      "Channel Index: 1\n",
      "-2.4690888\n",
      "-----\n",
      "Row Index: 6827\n",
      "Column Index: 14\n",
      "Channel Index: 2\n",
      "-2.4690888\n",
      "-----\n",
      "Row Index: 6827\n",
      "Column Index: 14\n",
      "Channel Index: 3\n",
      "-2.4690888\n",
      "-----\n",
      "Row Index: 6848\n",
      "Column Index: 4\n",
      "Channel Index: 0\n",
      "1.811855\n",
      "-----\n",
      "Row Index: 6848\n",
      "Column Index: 5\n",
      "Channel Index: 0\n",
      "-6.7841673\n",
      "-----\n",
      "Row Index: 6848\n",
      "Column Index: 5\n",
      "Channel Index: 1\n",
      "-6.7841673\n",
      "-----\n",
      "Row Index: 7950\n",
      "Column Index: 25\n",
      "Channel Index: 0\n",
      "-6.569183\n",
      "-----\n",
      "Row Index: 7950\n",
      "Column Index: 26\n",
      "Channel Index: 0\n",
      "1.7648252\n",
      "-----\n",
      "Row Index: 7950\n",
      "Column Index: 26\n",
      "Channel Index: 1\n",
      "1.7648252\n",
      "-----\n",
      "Row Index: 7950\n",
      "Column Index: 26\n",
      "Channel Index: 2\n",
      "1.7648252\n",
      "-----\n",
      "Row Index: 7950\n",
      "Column Index: 26\n",
      "Channel Index: 3\n",
      "1.7648252\n",
      "-----\n",
      "Row Index: 8166\n",
      "Column Index: 1\n",
      "Channel Index: 0\n",
      "-6.7523375\n",
      "-----\n",
      "Row Index: 8166\n",
      "Column Index: 2\n",
      "Channel Index: 0\n",
      "-6.7401752\n",
      "-----\n",
      "Row Index: 8166\n",
      "Column Index: 2\n",
      "Channel Index: 1\n",
      "-6.7401752\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "nan_indices_x_train = (\n",
    "    np.array([6610, 6610, 6610, 6780, 6780, 6780, 6780, 6780, 6827, 6827, 6827, 6827, 6827, 6848, 6848, 6848, 7950, 7950, 7950, 7950, 7950, 8166, 8166, 8166]),\n",
    "    np.array([18, 19, 19, 7, 8, 8, 8, 8, 13, 14, 14, 14, 14, 4, 5, 5, 25, 26, 26, 26, 26, 1, 2, 2]),\n",
    "    np.array([0, 0, 1, 0, 0, 1, 2, 3, 0, 0, 1, 2, 3, 0, 0, 1, 0, 0, 1, 2, 3, 0, 0, 1])\n",
    ")\n",
    "\n",
    "# Loop through the arrays in the tuple and print values\n",
    "for row_idx, col_idx, channel_idx in zip(*nan_indices_x_train):\n",
    "    print(\"Row Index:\", row_idx)\n",
    "    print(\"Column Index:\", col_idx)\n",
    "    print(\"Channel Index:\", channel_idx)\n",
    "    print(Z[row_idx][col_idx][channel_idx])\n",
    "    print(\"-----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4b1d776",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T21:04:48.347667Z",
     "start_time": "2023-08-30T21:04:48.329607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.811855 ,  1.811855 ,  1.8171737,  1.8224925,  1.8282486,\n",
       "        1.8300924,  1.83196  ,  1.8361086,  1.8402808, -2.4793954,\n",
       "       -6.801341 , -6.790644 , -6.7845454, -2.4615362,  1.859192 ,\n",
       "        1.8296788,  1.8002127,  1.7661372,  1.732109 ,  1.7097347,\n",
       "        1.6874195,  1.686273 ,  1.6851739,  1.7021347,  1.7191548,\n",
       "        1.7410799,  1.7630523,  1.7785594,  1.7941376,  1.8039596,\n",
       "        1.8138406,  1.8220788,  1.8303642,  1.8390634,  1.8478216,\n",
       "       -2.475235 , -6.800549 , -6.78464  , -6.7733054, -6.7471724,\n",
       "       -6.7233205, -6.684399 , -6.64543  , -6.6012135, -6.5569615,\n",
       "       -6.5198836, -6.4827704, -6.4544277, -6.4260373, -6.398486 ,\n",
       "       -6.370899 , -6.3406296, -6.310336 , -6.2823715, -6.2543945,\n",
       "       -6.225638 , -6.1968694, -6.1628175, -6.1287656, -6.092787 ,\n",
       "       -6.0568204, -6.026208 , -5.9955955, -5.9955955], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z[6848][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aba7d96b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T20:36:11.490011Z",
     "start_time": "2023-08-30T20:36:11.434156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN indices in x_train:\n",
      "(array([6610, 6610, 6610, 6780, 6780, 6780, 6780, 6780, 6827, 6827, 6827,\n",
      "       6827, 6827, 6848, 6848, 6848, 7950, 7950, 7950, 7950, 7950, 8166,\n",
      "       8166, 8166], dtype=int64), array([18, 19, 19,  7,  8,  8,  8,  8, 13, 14, 14, 14, 14,  4,  5,  5, 25,\n",
      "       26, 26, 26, 26,  1,  2,  2], dtype=int64), array([0, 0, 1, 0, 0, 1, 2, 3, 0, 0, 1, 2, 3, 0, 0, 1, 0, 0, 1, 2, 3, 0,\n",
      "       0, 1], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "nan_indices_Z = np.where(np.isnan(Z))\n",
    "print(\"NaN indices in x_train:\")\n",
    "print(nan_indices_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d668a5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T20:32:58.388683Z",
     "start_time": "2023-08-30T20:32:58.293618Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN indices in x_train:\n",
      "(array([6610, 6610, 6610, 6780, 6780, 6780, 6780, 6780, 6827, 6827, 6827,\n",
      "       6827, 6827, 6848, 6848, 6848, 7950, 7950, 7950, 7950, 7950, 8166,\n",
      "       8166, 8166], dtype=int64), array([18, 19, 19,  7,  8,  8,  8,  8, 13, 14, 14, 14, 14,  4,  5,  5, 25,\n",
      "       26, 26, 26, 26,  1,  2,  2], dtype=int64), array([0, 0, 1, 0, 0, 1, 2, 3, 0, 0, 1, 2, 3, 0, 0, 1, 0, 0, 1, 2, 3, 0,\n",
      "       0, 1], dtype=int64), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0], dtype=int64))\n",
      "NaN indices in y_train:\n",
      "(array([6598, 6598, 6598, 6768, 6768, 6768, 6768, 6768, 6815, 6815, 6815,\n",
      "       6815, 6815, 6836, 6836, 6836, 7938, 7938, 7938, 7938, 7938, 8154,\n",
      "       8154, 8154], dtype=int64), array([18, 19, 19,  7,  8,  8,  8,  8, 13, 14, 14, 14, 14,  4,  5,  5, 25,\n",
      "       26, 26, 26, 26,  1,  2,  2], dtype=int64), array([0, 0, 1, 0, 0, 1, 2, 3, 0, 0, 1, 2, 3, 0, 0, 1, 0, 0, 1, 2, 3, 0,\n",
      "       0, 1], dtype=int64), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0], dtype=int64))\n",
      "NaN indices in x_val:\n",
      "(array([], dtype=int64), array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))\n",
      "NaN indices in y_val:\n",
      "(array([], dtype=int64), array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# Find and print indices of NaN values in x_train\n",
    "nan_indices_x_train = np.where(np.isnan(x_train))\n",
    "print(\"NaN indices in x_train:\")\n",
    "print(nan_indices_x_train)\n",
    "\n",
    "# Find and print indices of NaN values in y_train\n",
    "nan_indices_y_train = np.where(np.isnan(y_train))\n",
    "print(\"NaN indices in y_train:\")\n",
    "print(nan_indices_y_train)\n",
    "\n",
    "# Find and print indices of NaN values in x_val\n",
    "nan_indices_x_val = np.where(np.isnan(x_val))\n",
    "print(\"NaN indices in x_val:\")\n",
    "print(nan_indices_x_val)\n",
    "\n",
    "# Find and print indices of NaN values in y_val\n",
    "nan_indices_y_val = np.where(np.isnan(y_val))\n",
    "print(\"NaN indices in y_val:\")\n",
    "print(nan_indices_y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59c31b23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T20:27:01.814951Z",
     "start_time": "2023-08-30T20:27:01.795954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8460, 32, 64, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b752225",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T20:27:10.010486Z",
     "start_time": "2023-08-30T20:27:10.000400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8460, 32, 64, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b417b55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T20:26:39.167374Z",
     "start_time": "2023-08-30T20:26:39.155375Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(276, 32, 64, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d739d1b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T20:26:54.945307Z",
     "start_time": "2023-08-30T20:26:54.940309Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(276, 32, 64, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01c14fd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T20:23:46.110064Z",
     "start_time": "2023-08-30T20:23:45.996548Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15444\\1949693125.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x_val' is not defined"
     ]
    }
   ],
   "source": [
    "x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5704842d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-31T22:03:10.457130Z",
     "start_time": "2023-08-30T22:09:04.552377Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|                                                                         | 0/42 [00:00<?, ?it/s]D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "  after removing the cwd from sys.path.\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "  import sys\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "  \n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** counter************* 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:32: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (2, 2), activation=\"relu\", padding=\"same\")`\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:36: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (2, 2), activation=\"relu\", padding=\"same\")`\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:37: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (5, 5), activation=\"linear\", padding=\"same\")`\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:44: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 142s 17ms/step - loss: 4.0010 - val_loss: 0.7709\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.77087, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 146s 17ms/step - loss: 3.9174 - val_loss: 0.7360\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.77087 to 0.73603, saving model to best_weights_lead12.h5\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 147s 17ms/step - loss: 3.7878 - val_loss: 0.8454\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.73603\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 149s 18ms/step - loss: 3.7374 - val_loss: 0.8584\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.73603\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 151s 18ms/step - loss: 3.6514 - val_loss: 0.8851\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.73603\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 156s 18ms/step - loss: 3.5694 - val_loss: 0.8042\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.73603\n",
      "Epoch 7/20\n",
      "8460/8460 [==============================] - 154s 18ms/step - loss: 3.4738 - val_loss: 0.9338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:   2%|█▍                                                          | 1/42 [17:27<11:55:37, 1047.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00007: val_loss did not improve from 0.73603\n",
      "******************** counter************* 2\n",
      "Train on 8484 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8484/8484 [==============================] - 152s 18ms/step - loss: 2.1476 - val_loss: 1.2994\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.29942, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8484/8484 [==============================] - 151s 18ms/step - loss: 2.0732 - val_loss: 1.3515\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.29942\n",
      "Epoch 3/20\n",
      "8484/8484 [==============================] - 159s 19ms/step - loss: 2.0241 - val_loss: 1.2476\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.29942 to 1.24763, saving model to best_weights_lead12.h5\n",
      "Epoch 4/20\n",
      "8484/8484 [==============================] - 156s 18ms/step - loss: 1.9795 - val_loss: 1.2462\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.24763 to 1.24619, saving model to best_weights_lead12.h5\n",
      "Epoch 5/20\n",
      "8484/8484 [==============================] - 155s 18ms/step - loss: 1.9180 - val_loss: 1.3078\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.24619\n",
      "Epoch 6/20\n",
      "8484/8484 [==============================] - 154s 18ms/step - loss: 1.8845 - val_loss: 1.2840\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.24619\n",
      "Epoch 7/20\n",
      "8484/8484 [==============================] - 154s 18ms/step - loss: 1.8166 - val_loss: 1.3286\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.24619\n",
      "Epoch 8/20\n",
      "8484/8484 [==============================] - 158s 19ms/step - loss: 1.7713 - val_loss: 1.3217\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.24619\n",
      "Epoch 9/20\n",
      "8484/8484 [==============================] - 161s 19ms/step - loss: 1.7147 - val_loss: 1.3984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:   5%|██▊                                                         | 2/42 [40:50<13:57:45, 1256.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00009: val_loss did not improve from 1.24619\n",
      "******************** counter************* 3\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 156s 18ms/step - loss: 2.0428 - val_loss: 0.7813\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.78134, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 156s 18ms/step - loss: 1.9539 - val_loss: 0.6108\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.78134 to 0.61080, saving model to best_weights_lead12.h5\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 155s 18ms/step - loss: 1.9070 - val_loss: 0.7263\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.61080\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 151s 18ms/step - loss: 1.8502 - val_loss: 0.8362\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.61080\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 153s 18ms/step - loss: 1.8049 - val_loss: 0.7265\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.61080\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 157s 19ms/step - loss: 1.7533 - val_loss: 0.6949\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61080\n",
      "Epoch 7/20\n",
      "8460/8460 [==============================] - 155s 18ms/step - loss: 1.6980 - val_loss: 0.8240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:   7%|████▎                                                       | 3/42 [58:57<12:46:26, 1179.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61080\n",
      "******************** counter************* 4\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 162s 19ms/step - loss: 2.7523 - val_loss: 1.0127\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01274, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 164s 19ms/step - loss: 2.6083 - val_loss: 1.0634\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01274\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 158s 19ms/step - loss: 2.5175 - val_loss: 1.1019\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01274\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 156s 18ms/step - loss: 2.4395 - val_loss: 0.9205\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01274 to 0.92051, saving model to best_weights_lead12.h5\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 157s 19ms/step - loss: 2.3458 - val_loss: 0.8886\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.92051 to 0.88865, saving model to best_weights_lead12.h5\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 155s 18ms/step - loss: 2.3156 - val_loss: 0.8985\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.88865\n",
      "Epoch 7/20\n",
      "8460/8460 [==============================] - 158s 19ms/step - loss: 2.2405 - val_loss: 0.8650\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.88865 to 0.86495, saving model to best_weights_lead12.h5\n",
      "Epoch 8/20\n",
      "8460/8460 [==============================] - 167s 20ms/step - loss: 2.1437 - val_loss: 0.9539\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.86495\n",
      "Epoch 9/20\n",
      "8460/8460 [==============================] - 166s 20ms/step - loss: 2.0893 - val_loss: 0.9267\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.86495\n",
      "Epoch 10/20\n",
      "8460/8460 [==============================] - 180s 21ms/step - loss: 2.0374 - val_loss: 0.9158\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.86495\n",
      "Epoch 11/20\n",
      "8460/8460 [==============================] - 184s 22ms/step - loss: 1.9715 - val_loss: 0.9514\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.86495\n",
      "Epoch 12/20\n",
      "8460/8460 [==============================] - 178s 21ms/step - loss: 1.9117 - val_loss: 0.9045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  10%|█████▌                                                    | 4/42 [1:32:05<15:49:10, 1498.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00012: val_loss did not improve from 0.86495\n",
      "******************** counter************* 5\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 179s 21ms/step - loss: 1.9306 - val_loss: 2.1092\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.10924, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 177s 21ms/step - loss: 1.7971 - val_loss: 2.1875\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.10924\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 170s 20ms/step - loss: 1.7155 - val_loss: 2.2175\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.10924\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 166s 20ms/step - loss: 1.6329 - val_loss: 2.0908\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.10924 to 2.09078, saving model to best_weights_lead12.h5\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 171s 20ms/step - loss: 1.5894 - val_loss: 2.3486\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.09078\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 311s 37ms/step - loss: 1.5503 - val_loss: 2.2153\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.09078\n",
      "Epoch 7/20\n",
      "8460/8460 [==============================] - 367s 43ms/step - loss: 1.5005 - val_loss: 2.4295\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.09078\n",
      "Epoch 8/20\n",
      "8460/8460 [==============================] - 368s 43ms/step - loss: 1.4571 - val_loss: 2.1344\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.09078\n",
      "Epoch 9/20\n",
      "8460/8460 [==============================] - 366s 43ms/step - loss: 1.4183 - val_loss: 2.2842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  12%|██████▉                                                   | 5/42 [2:10:04<18:17:32, 1779.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00009: val_loss did not improve from 2.09078\n",
      "******************** counter************* 6\n",
      "Train on 8484 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8484/8484 [==============================] - 384s 45ms/step - loss: 1.8174 - val_loss: 1.0114\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01141, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8484/8484 [==============================] - 379s 45ms/step - loss: 1.7278 - val_loss: 1.0785\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.01141\n",
      "Epoch 3/20\n",
      "8484/8484 [==============================] - 384s 45ms/step - loss: 1.6712 - val_loss: 0.9368\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.01141 to 0.93680, saving model to best_weights_lead12.h5\n",
      "Epoch 4/20\n",
      "8484/8484 [==============================] - 384s 45ms/step - loss: 1.6351 - val_loss: 1.0635\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.93680\n",
      "Epoch 5/20\n",
      "8484/8484 [==============================] - 380s 45ms/step - loss: 1.5730 - val_loss: 0.9199\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.93680 to 0.91993, saving model to best_weights_lead12.h5\n",
      "Epoch 6/20\n",
      "8484/8484 [==============================] - 381s 45ms/step - loss: 1.5361 - val_loss: 0.9469\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.91993\n",
      "Epoch 7/20\n",
      "8484/8484 [==============================] - 376s 44ms/step - loss: 1.4863 - val_loss: 0.9778\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.91993\n",
      "Epoch 8/20\n",
      "8484/8484 [==============================] - 380s 45ms/step - loss: 1.4609 - val_loss: 1.1194\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.91993\n",
      "Epoch 9/20\n",
      "8484/8484 [==============================] - 376s 44ms/step - loss: 1.4155 - val_loss: 1.0417\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.91993\n",
      "Epoch 10/20\n",
      "8484/8484 [==============================] - 380s 45ms/step - loss: 1.3692 - val_loss: 1.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  14%|████████▎                                                 | 6/42 [3:13:37<24:42:48, 2471.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00010: val_loss did not improve from 0.91993\n",
      "******************** counter************* 7\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 389s 46ms/step - loss: 2.8064 - val_loss: 0.4604\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.46035, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 384s 45ms/step - loss: 2.6673 - val_loss: 0.5721\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.46035\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 387s 46ms/step - loss: 2.5706 - val_loss: 0.5056\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.46035\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 382s 45ms/step - loss: 2.5060 - val_loss: 0.4894\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.46035\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 385s 45ms/step - loss: 2.4460 - val_loss: 0.5050\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.46035\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 389s 46ms/step - loss: 2.3768 - val_loss: 0.4900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  17%|█████████▋                                                | 7/42 [3:52:25<23:34:07, 2424.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: val_loss did not improve from 0.46035\n",
      "******************** counter************* 8\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 399s 47ms/step - loss: 2.0845 - val_loss: 0.8869\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.88689, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 394s 47ms/step - loss: 1.9896 - val_loss: 1.0247\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.88689\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 395s 47ms/step - loss: 1.9301 - val_loss: 0.9168\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.88689\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 390s 46ms/step - loss: 1.8848 - val_loss: 0.8848\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.88689 to 0.88481, saving model to best_weights_lead12.h5\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 395s 47ms/step - loss: 1.8294 - val_loss: 0.9380\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.88481\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 395s 47ms/step - loss: 1.7872 - val_loss: 0.9407\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.88481\n",
      "Epoch 7/20\n",
      "8460/8460 [==============================] - 391s 46ms/step - loss: 1.7507 - val_loss: 0.9749\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.88481\n",
      "Epoch 8/20\n",
      "8460/8460 [==============================] - 395s 47ms/step - loss: 1.7078 - val_loss: 0.9413\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.88481\n",
      "Epoch 9/20\n",
      "8460/8460 [==============================] - 394s 47ms/step - loss: 1.6844 - val_loss: 1.0462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  19%|███████████                                               | 8/42 [4:51:45<26:18:45, 2786.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00009: val_loss did not improve from 0.88481\n",
      "******************** counter************* 9\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 404s 48ms/step - loss: 4.4300 - val_loss: 6.3056\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6.30563, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 400s 47ms/step - loss: 4.2887 - val_loss: 6.5145\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 6.30563\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 403s 48ms/step - loss: 4.1458 - val_loss: 6.6342\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 6.30563\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 398s 47ms/step - loss: 4.0194 - val_loss: 7.2257\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 6.30563\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 403s 48ms/step - loss: 3.9465 - val_loss: 7.1770\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 6.30563\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 402s 48ms/step - loss: 3.8400 - val_loss: 6.6851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  21%|████████████▍                                             | 9/42 [5:32:10<24:30:08, 2672.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: val_loss did not improve from 6.30563\n",
      "******************** counter************* 10\n",
      "Train on 8484 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8484/8484 [==============================] - 416s 49ms/step - loss: 1.6653 - val_loss: 1.2863\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.28626, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8484/8484 [==============================] - 417s 49ms/step - loss: 1.5633 - val_loss: 1.2969\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.28626\n",
      "Epoch 3/20\n",
      "8484/8484 [==============================] - 419s 49ms/step - loss: 1.5131 - val_loss: 1.1995\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.28626 to 1.19952, saving model to best_weights_lead12.h5\n",
      "Epoch 4/20\n",
      "8484/8484 [==============================] - 419s 49ms/step - loss: 1.4719 - val_loss: 1.3996\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.19952\n",
      "Epoch 5/20\n",
      "8484/8484 [==============================] - 413s 49ms/step - loss: 1.4333 - val_loss: 1.3181\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.19952\n",
      "Epoch 6/20\n",
      "8484/8484 [==============================] - 416s 49ms/step - loss: 1.3977 - val_loss: 1.3160\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.19952\n",
      "Epoch 7/20\n",
      "8484/8484 [==============================] - 415s 49ms/step - loss: 1.3734 - val_loss: 1.3048\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.19952\n",
      "Epoch 8/20\n",
      "8484/8484 [==============================] - 415s 49ms/step - loss: 1.3317 - val_loss: 1.4533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  24%|█████████████▌                                           | 10/42 [6:27:53<25:35:52, 2879.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00008: val_loss did not improve from 1.19952\n",
      "******************** counter************* 11\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 430s 51ms/step - loss: 1.7930 - val_loss: 0.9356\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.93565, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 423s 50ms/step - loss: 1.6984 - val_loss: 0.9408\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.93565\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 429s 51ms/step - loss: 1.6439 - val_loss: 0.8645\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.93565 to 0.86450, saving model to best_weights_lead12.h5\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 427s 51ms/step - loss: 1.5992 - val_loss: 1.0590\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.86450\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 425s 50ms/step - loss: 1.5356 - val_loss: 0.9850\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.86450\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 424s 50ms/step - loss: 1.4840 - val_loss: 0.9858\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.86450\n",
      "Epoch 7/20\n",
      "8460/8460 [==============================] - 428s 51ms/step - loss: 1.4533 - val_loss: 1.0115\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.86450\n",
      "Epoch 8/20\n",
      "8460/8460 [==============================] - 427s 51ms/step - loss: 1.3971 - val_loss: 1.0780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  26%|██████████████▉                                          | 11/42 [7:25:02<26:14:52, 3048.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00008: val_loss did not improve from 0.86450\n",
      "******************** counter************* 12\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 440s 52ms/step - loss: 1.6658 - val_loss: 1.1394\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.13942, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 436s 52ms/step - loss: 1.6042 - val_loss: 0.9989\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.13942 to 0.99891, saving model to best_weights_lead12.h5\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 438s 52ms/step - loss: 1.5629 - val_loss: 1.0303\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.99891\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 437s 52ms/step - loss: 1.5315 - val_loss: 1.2524\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.99891\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 439s 52ms/step - loss: 1.5003 - val_loss: 1.0609\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.99891\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 440s 52ms/step - loss: 1.4596 - val_loss: 1.0407\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.99891\n",
      "Epoch 7/20\n",
      "8460/8460 [==============================] - 441s 52ms/step - loss: 1.4196 - val_loss: 1.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  29%|████████████████▎                                        | 12/42 [8:16:32<25:30:23, 3060.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99891\n",
      "******************** counter************* 13\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 448s 53ms/step - loss: 2.3503 - val_loss: 2.2256\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.22556, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 442s 52ms/step - loss: 2.2616 - val_loss: 2.5115\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.22556\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 441s 52ms/step - loss: 2.2109 - val_loss: 2.2430\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.22556\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 441s 52ms/step - loss: 2.1579 - val_loss: 2.6073\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.22556\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 443s 52ms/step - loss: 2.1083 - val_loss: 2.2168\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.22556 to 2.21684, saving model to best_weights_lead12.h5\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 442s 52ms/step - loss: 2.0568 - val_loss: 2.6120\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.21684\n",
      "Epoch 7/20\n",
      "8460/8460 [==============================] - 445s 53ms/step - loss: 2.0162 - val_loss: 2.3676\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.21684\n",
      "Epoch 8/20\n",
      "8460/8460 [==============================] - 442s 52ms/step - loss: 1.9631 - val_loss: 2.7485\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.21684\n",
      "Epoch 9/20\n",
      "8460/8460 [==============================] - 443s 52ms/step - loss: 1.9086 - val_loss: 2.2805\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.21684\n",
      "Epoch 10/20\n",
      "8460/8460 [==============================] - 441s 52ms/step - loss: 1.8628 - val_loss: 2.4322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  31%|█████████████████▋                                       | 13/42 [9:30:37<28:02:03, 3480.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00010: val_loss did not improve from 2.21684\n",
      "******************** counter************* 14\n",
      "Train on 8484 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8484/8484 [==============================] - 465s 55ms/step - loss: 1.7646 - val_loss: 0.6229\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.62287, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8484/8484 [==============================] - 465s 55ms/step - loss: 1.6872 - val_loss: 0.6463\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.62287\n",
      "Epoch 3/20\n",
      "8484/8484 [==============================] - 467s 55ms/step - loss: 1.6437 - val_loss: 0.6534\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.62287\n",
      "Epoch 4/20\n",
      "8484/8484 [==============================] - 464s 55ms/step - loss: 1.6069 - val_loss: 0.6699\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.62287\n",
      "Epoch 5/20\n",
      "8484/8484 [==============================] - 464s 55ms/step - loss: 1.5667 - val_loss: 0.6258\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62287\n",
      "Epoch 6/20\n",
      "8484/8484 [==============================] - 463s 55ms/step - loss: 1.5425 - val_loss: 0.5632\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62287 to 0.56316, saving model to best_weights_lead12.h5\n",
      "Epoch 7/20\n",
      "8484/8484 [==============================] - 466s 55ms/step - loss: 1.5021 - val_loss: 0.7481\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.56316\n",
      "Epoch 8/20\n",
      "8484/8484 [==============================] - 470s 55ms/step - loss: 1.4620 - val_loss: 0.6719\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.56316\n",
      "Epoch 9/20\n",
      "8484/8484 [==============================] - 469s 55ms/step - loss: 1.4315 - val_loss: 0.6141\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.56316\n",
      "Epoch 10/20\n",
      "8484/8484 [==============================] - 471s 56ms/step - loss: 1.4053 - val_loss: 0.6868\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.56316\n",
      "Epoch 11/20\n",
      "8484/8484 [==============================] - 471s 55ms/step - loss: 1.3676 - val_loss: 0.6330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  33%|██████████████████▋                                     | 14/42 [10:56:31<30:59:56, 3985.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00011: val_loss did not improve from 0.56316\n",
      "******************** counter************* 15\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 482s 57ms/step - loss: 1.7892 - val_loss: 0.9819\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.98189, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 473s 56ms/step - loss: 1.6795 - val_loss: 0.5652\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.98189 to 0.56518, saving model to best_weights_lead12.h5\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 471s 56ms/step - loss: 1.6246 - val_loss: 0.8912\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.56518\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 469s 55ms/step - loss: 1.5622 - val_loss: 0.6753\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.56518\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 469s 55ms/step - loss: 1.5176 - val_loss: 0.7181\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.56518\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 472s 56ms/step - loss: 1.4780 - val_loss: 0.5443\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.56518 to 0.54429, saving model to best_weights_lead12.h5\n",
      "Epoch 7/20\n",
      "8460/8460 [==============================] - 472s 56ms/step - loss: 1.4363 - val_loss: 0.6686\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.54429\n",
      "Epoch 8/20\n",
      "8460/8460 [==============================] - 474s 56ms/step - loss: 1.3854 - val_loss: 0.7308\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.54429\n",
      "Epoch 9/20\n",
      "8460/8460 [==============================] - 473s 56ms/step - loss: 1.3614 - val_loss: 0.6563\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.54429\n",
      "Epoch 10/20\n",
      "8460/8460 [==============================] - 473s 56ms/step - loss: 1.3313 - val_loss: 0.6472\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.54429\n",
      "Epoch 11/20\n",
      "8460/8460 [==============================] - 472s 56ms/step - loss: 1.3063 - val_loss: 0.6688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  36%|████████████████████                                    | 15/42 [12:23:35<32:41:29, 4358.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00011: val_loss did not improve from 0.54429\n",
      "******************** counter************* 16\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 450s 53ms/step - loss: 1.6798 - val_loss: 0.9127\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.91267, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 483s 57ms/step - loss: 1.5913 - val_loss: 0.8295\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.91267 to 0.82948, saving model to best_weights_lead12.h5\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 482s 57ms/step - loss: 1.5456 - val_loss: 0.8310\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.82948\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 481s 57ms/step - loss: 1.5010 - val_loss: 0.8997\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.82948\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 480s 57ms/step - loss: 1.4692 - val_loss: 0.9189\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.82948\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 482s 57ms/step - loss: 1.4409 - val_loss: 0.8957\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.82948\n",
      "Epoch 7/20\n",
      "8460/8460 [==============================] - 484s 57ms/step - loss: 1.4086 - val_loss: 0.8504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  38%|█████████████████████▎                                  | 16/42 [13:19:40<29:19:15, 4059.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00007: val_loss did not improve from 0.82948\n",
      "******************** counter************* 17\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 502s 59ms/step - loss: 2.4618 - val_loss: 0.5986\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.59864, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 497s 59ms/step - loss: 2.3643 - val_loss: 0.7818\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.59864\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 500s 59ms/step - loss: 2.3000 - val_loss: 1.0814\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.59864\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 501s 59ms/step - loss: 2.2491 - val_loss: 0.9855\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.59864\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 502s 59ms/step - loss: 2.2049 - val_loss: 0.8361\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.59864\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 502s 59ms/step - loss: 2.1692 - val_loss: 0.8342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  40%|██████████████████████▋                                 | 17/42 [14:10:07<26:02:10, 3749.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: val_loss did not improve from 0.59864\n",
      "******************** counter************* 18\n",
      "Train on 8484 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8484/8484 [==============================] - 513s 60ms/step - loss: 3.1703 - val_loss: 0.6448\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64482, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8484/8484 [==============================] - 502s 59ms/step - loss: 3.0547 - val_loss: 0.7065\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.64482\n",
      "Epoch 3/20\n",
      "8484/8484 [==============================] - 486s 57ms/step - loss: 2.9619 - val_loss: 0.8051\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.64482\n",
      "Epoch 4/20\n",
      "8484/8484 [==============================] - 501s 59ms/step - loss: 2.8913 - val_loss: 0.7192\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.64482\n",
      "Epoch 5/20\n",
      "8484/8484 [==============================] - 499s 59ms/step - loss: 2.8386 - val_loss: 0.8911\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.64482\n",
      "Epoch 6/20\n",
      "8484/8484 [==============================] - 498s 59ms/step - loss: 2.7614 - val_loss: 0.6669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  43%|████████████████████████                                | 18/42 [15:00:31<23:32:28, 3531.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: val_loss did not improve from 0.64482\n",
      "******************** counter************* 19\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 510s 60ms/step - loss: 2.0391 - val_loss: 0.7200\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.72001, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 515s 61ms/step - loss: 1.9211 - val_loss: 0.5940\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.72001 to 0.59402, saving model to best_weights_lead12.h5\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 517s 61ms/step - loss: 1.8542 - val_loss: 0.6516\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.59402\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 518s 61ms/step - loss: 1.8129 - val_loss: 0.6688\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.59402\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 517s 61ms/step - loss: 1.7835 - val_loss: 0.7075\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.59402\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 516s 61ms/step - loss: 1.7165 - val_loss: 0.6292\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.59402\n",
      "Epoch 7/20\n",
      "8460/8460 [==============================] - 517s 61ms/step - loss: 1.6766 - val_loss: 0.6210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  45%|█████████████████████████▎                              | 19/42 [16:01:07<22:45:44, 3562.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00007: val_loss did not improve from 0.59402\n",
      "******************** counter************* 20\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 525s 62ms/step - loss: 1.3306 - val_loss: 0.9058\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.90579, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 516s 61ms/step - loss: 1.2635 - val_loss: 0.9763\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.90579\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 516s 61ms/step - loss: 1.2277 - val_loss: 1.0510\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.90579\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 516s 61ms/step - loss: 1.1832 - val_loss: 0.9196\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.90579\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 514s 61ms/step - loss: 1.1466 - val_loss: 1.0287\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.90579\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 513s 61ms/step - loss: 1.1201 - val_loss: 0.9975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  48%|██████████████████████████▋                             | 20/42 [16:53:16<20:58:33, 3432.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: val_loss did not improve from 0.90579\n",
      "******************** counter************* 21\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 534s 63ms/step - loss: 1.6193 - val_loss: 0.5961\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.59611, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 531s 63ms/step - loss: 1.5558 - val_loss: 0.6093\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.59611\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 530s 63ms/step - loss: 1.5247 - val_loss: 0.6667\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.59611\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 526s 62ms/step - loss: 1.4836 - val_loss: 0.6253\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.59611\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 530s 63ms/step - loss: 1.4487 - val_loss: 0.6435\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.59611\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 527s 62ms/step - loss: 1.4127 - val_loss: 0.6350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  50%|████████████████████████████                            | 21/42 [17:46:45<19:37:55, 3365.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: val_loss did not improve from 0.59611\n",
      "******************** counter************* 22\n",
      "Train on 8484 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8484/8484 [==============================] - 532s 63ms/step - loss: 1.2741 - val_loss: 0.6760\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.67602, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8484/8484 [==============================] - 523s 62ms/step - loss: 1.2106 - val_loss: 0.5217\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.67602 to 0.52168, saving model to best_weights_lead12.h5\n",
      "Epoch 3/20\n",
      "8484/8484 [==============================] - 528s 62ms/step - loss: 1.1884 - val_loss: 0.7344\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.52168\n",
      "Epoch 4/20\n",
      "8484/8484 [==============================] - 535s 63ms/step - loss: 1.1831 - val_loss: 0.6125\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.52168\n",
      "Epoch 5/20\n",
      "8484/8484 [==============================] - 538s 63ms/step - loss: 1.1360 - val_loss: 0.5418\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.52168\n",
      "Epoch 6/20\n",
      "8484/8484 [==============================] - 547s 64ms/step - loss: 1.0978 - val_loss: 0.6963\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.52168\n",
      "Epoch 7/20\n",
      "8484/8484 [==============================] - 548s 65ms/step - loss: 1.0821 - val_loss: 0.5576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  52%|█████████████████████████████▎                          | 22/42 [18:49:48<19:23:34, 3490.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00007: val_loss did not improve from 0.52168\n",
      "******************** counter************* 23\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 564s 67ms/step - loss: 1.8820 - val_loss: 1.2167\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.21670, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 555s 66ms/step - loss: 1.8265 - val_loss: 1.8535\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.21670\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 555s 66ms/step - loss: 1.7863 - val_loss: 1.3329\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.21670\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 555s 66ms/step - loss: 1.7663 - val_loss: 1.2559\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.21670\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 557s 66ms/step - loss: 1.7349 - val_loss: 1.2869\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.21670\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 554s 65ms/step - loss: 1.7144 - val_loss: 1.4054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  55%|██████████████████████████████▋                         | 23/42 [19:46:02<18:14:19, 3455.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: val_loss did not improve from 1.21670\n",
      "******************** counter************* 24\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 583s 69ms/step - loss: 1.9269 - val_loss: 0.4988\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49882, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 571s 68ms/step - loss: 1.8649 - val_loss: 0.5179\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.49882\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 573s 68ms/step - loss: 1.8260 - val_loss: 0.5470\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.49882\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 573s 68ms/step - loss: 1.7969 - val_loss: 0.4995\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.49882\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 572s 68ms/step - loss: 1.7580 - val_loss: 0.5352\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.49882\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 571s 68ms/step - loss: 1.7255 - val_loss: 0.6318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  57%|████████████████████████████████                        | 24/42 [20:44:03<17:18:58, 3463.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: val_loss did not improve from 0.49882\n",
      "******************** counter************* 25\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 580s 69ms/step - loss: 1.3833 - val_loss: 0.8817\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.88167, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 574s 68ms/step - loss: 1.3152 - val_loss: 0.8734\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.88167 to 0.87337, saving model to best_weights_lead12.h5\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 573s 68ms/step - loss: 1.2749 - val_loss: 1.0027\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.87337\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 573s 68ms/step - loss: 1.2333 - val_loss: 1.0226\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.87337\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 573s 68ms/step - loss: 1.2681 - val_loss: 0.8870\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.87337\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 569s 67ms/step - loss: 1.2027 - val_loss: 0.8630\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.87337 to 0.86300, saving model to best_weights_lead12.h5\n",
      "Epoch 7/20\n",
      "8460/8460 [==============================] - 567s 67ms/step - loss: 1.1831 - val_loss: 0.8925\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.86300\n",
      "Epoch 8/20\n",
      "8460/8460 [==============================] - 574s 68ms/step - loss: 1.1483 - val_loss: 0.8893\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.86300\n",
      "Epoch 9/20\n",
      "8460/8460 [==============================] - 569s 67ms/step - loss: 1.1240 - val_loss: 0.9556\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.86300\n",
      "Epoch 10/20\n",
      "8460/8460 [==============================] - 572s 68ms/step - loss: 1.0831 - val_loss: 0.9636\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.86300\n",
      "Epoch 11/20\n",
      "8460/8460 [==============================] - 568s 67ms/step - loss: 1.0684 - val_loss: 0.8645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  60%|█████████████████████████████████▎                      | 25/42 [22:29:36<20:25:13, 4324.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00011: val_loss did not improve from 0.86300\n",
      "******************** counter************* 26\n",
      "Train on 8484 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8484/8484 [==============================] - 588s 69ms/step - loss: 1.9170 - val_loss: 1.6491\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.64907, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8484/8484 [==============================] - 580s 68ms/step - loss: 1.8590 - val_loss: 1.7832\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.64907\n",
      "Epoch 3/20\n",
      "8484/8484 [==============================] - 586s 69ms/step - loss: 1.8352 - val_loss: 1.7033\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.64907\n",
      "Epoch 4/20\n",
      "8484/8484 [==============================] - 585s 69ms/step - loss: 1.8030 - val_loss: 1.6535\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.64907\n",
      "Epoch 5/20\n",
      "8484/8484 [==============================] - 587s 69ms/step - loss: 1.7719 - val_loss: 1.7608\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.64907\n",
      "Epoch 6/20\n",
      "8484/8484 [==============================] - 582s 69ms/step - loss: 1.7405 - val_loss: 1.8254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  62%|██████████████████████████████████▋                     | 26/42 [23:28:46<18:11:11, 4091.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: val_loss did not improve from 1.64907\n",
      "******************** counter************* 27\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 600s 71ms/step - loss: 2.5096 - val_loss: 0.8686\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.86855, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 594s 70ms/step - loss: 2.4404 - val_loss: 1.2330\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.86855\n",
      "Epoch 3/20\n",
      "4120/8460 [=============>................] - ETA: 4:57 - loss: 2.4373"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  62%|██████████████████████████████████▋                     | 26/42 [23:54:05<14:42:31, 3309.45s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26088\\2984525146.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m                        keras.callbacks.ModelCheckpoint('best_weights_lead'+str(lead)+'.h5', monitor='val_loss',\n\u001b[0;32m     47\u001b[0m                                                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m                                                     save_weights_only=True, mode='auto', period=1),history]\n\u001b[0m\u001b[0;32m     49\u001b[0m              )\n\u001b[0;32m     50\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\py37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\py37\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\py37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\py37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "num_epochs = 8\n",
    "lead=12\n",
    "count=0\n",
    "for loop in tqdm(fileList_train, desc=\"Processing files\"):\n",
    "    print('******************** counter*************',count)\n",
    "    File=nc.Dataset(loop)\n",
    "    Z=np.asarray(File['t'])\n",
    "    \n",
    "# # Create a mask to filter out samples with NaN values\n",
    "#     mask = np.all(~np.isnan(Z), axis=(1, 2))\n",
    "#     Z = Z[mask]\n",
    "    \n",
    "    trainN=np.size(Z,0)-300\n",
    "    Z=(Z-M)/sdev\n",
    "    \n",
    "    \n",
    "    x_train=Z[0:trainN,:,:]\n",
    "    x_train=x_train.reshape([np.size(x_train,0),32,64,1])\n",
    "    y_train=Z[lead:trainN+lead,:,:]\n",
    "    y_train=y_train.reshape([np.size(y_train,0),32,64,1])\n",
    "    \n",
    "    x_val= Z[trainN+lead:np.size(Z,0)-lead,:,:]\n",
    "    x_val=x_val.reshape([np.size(x_val,0),32,64,1])\n",
    "    \n",
    "    y_val= Z[trainN+lead*2:np.size(Z,0),:,:]\n",
    "    y_val=y_val.reshape([np.size(y_val,0),32,64,1])\n",
    "\n",
    "    if (count>0):\n",
    "        \n",
    "        model = stn()\n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "        # Modify the HDF5 file before loading weights\n",
    "#         modify_h5_file('best_weights_lead12.h5')\n",
    "\n",
    "        model.load_weights('best_weights_lead12.h5')\n",
    "        hist = model.fit(x_train, y_train,\n",
    "                       batch_size = batch_size,\n",
    "             verbose=1,\n",
    "             epochs = 20,\n",
    "             validation_data=(x_val,y_val),shuffle=True,\n",
    "             callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                        min_delta=0,\n",
    "                                        patience=5, # just to make sure we use a lot of patience before stopping\n",
    "                                        verbose=0, mode='auto'),\n",
    "                       keras.callbacks.ModelCheckpoint('best_weights_lead'+str(lead)+'.h5', monitor='val_loss',\n",
    "                                                    verbose=1, save_best_only=True,\n",
    "                                                    save_weights_only=True, mode='auto', period=1),history]\n",
    "             )\n",
    "        losses.extend(hist.history['loss'])\n",
    "        val_losses.extend(hist.history['val_loss'])\n",
    "\n",
    "    else:\n",
    "        hist = model.fit(x_train, y_train,\n",
    "                       batch_size = batch_size,\n",
    "             verbose=1,\n",
    "             epochs = 20,\n",
    "             validation_data=(x_val,y_val),shuffle=True,\n",
    "             callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                        min_delta=0,\n",
    "                                        patience=5, # just to make sure we use a lot of patience before stopping\n",
    "                                        verbose=0, mode='auto'),\n",
    "                       keras.callbacks.ModelCheckpoint('best_weights_lead'+str(lead)+'.h5', monitor='val_loss',\n",
    "                                                    verbose=1, save_best_only=True,\n",
    "                                                    save_weights_only=True, mode='auto', period=1),history]\n",
    "             )\n",
    "        losses.extend(hist.history['loss'])\n",
    "        val_losses.extend(hist.history['val_loss'])\n",
    "\n",
    "\n",
    "    count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "173eb549",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-31T22:06:49.011884Z",
     "start_time": "2023-08-31T22:06:48.996400Z"
    }
   },
   "outputs": [],
   "source": [
    "fileList_train = []\n",
    "fileList_validation =[]\n",
    "fileList_test=[]\n",
    "input_folder_path = folder_utils.find_folder(\n",
    "    country, data_folder, data_save_category, output_folder\n",
    ")\n",
    "\n",
    "for year in range (2004,2021):\n",
    "    file_path =  os.path.join(input_folder_path, f\"era5_pressure_level_{year}_regrid_filter_850.nc\")\n",
    "    fileList_train.append (file_path)\n",
    "    \n",
    "fileList_validation.append(os.path.join(input_folder_path, f\"era5_pressure_level_2021_regrid_filter_850.nc\"))\n",
    "fileList_test.append(os.path.join(input_folder_path, f\"era5_pressure_level_2022_regrid_filter_850.nc\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13a5e130",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-31T22:07:01.435688Z",
     "start_time": "2023-08-31T22:07:01.420239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2004_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2005_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2006_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2007_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2008_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2009_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2010_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2011_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2012_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2013_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2014_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2015_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2016_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2017_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2018_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2019_regrid_filter_850.nc',\n",
       " 'F:\\\\JuPyterNotebook\\\\irp_ww721_bakcup\\\\data\\\\processed_data\\\\ERA5_DATA\\\\GB_ERA5_DATA\\\\era5_pressure_level_2020_regrid_filter_850.nc']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileList_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1503b0d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T03:17:14.741076Z",
     "start_time": "2023-08-31T22:10:12.242475Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:   0%|                                                                         | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** counter************* 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "  after removing the cwd from sys.path.\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "  import sys\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "  \n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:32: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (2, 2), activation=\"relu\", padding=\"same\")`\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:36: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (2, 2), activation=\"relu\", padding=\"same\")`\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:37: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (5, 5), activation=\"linear\", padding=\"same\")`\n",
      "D:\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:44: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8484 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8484/8484 [==============================] - 639s 75ms/step - loss: 1.8944 - val_loss: 1.7717\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.77169, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8484/8484 [==============================] - 628s 74ms/step - loss: 1.8414 - val_loss: 1.6745\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.77169 to 1.67449, saving model to best_weights_lead12.h5\n",
      "Epoch 3/20\n",
      "8484/8484 [==============================] - 627s 74ms/step - loss: 1.7953 - val_loss: 1.6300\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.67449 to 1.62995, saving model to best_weights_lead12.h5\n",
      "Epoch 4/20\n",
      "8484/8484 [==============================] - 612s 72ms/step - loss: 1.7598 - val_loss: 1.6587\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.62995\n",
      "Epoch 5/20\n",
      "8484/8484 [==============================] - 619s 73ms/step - loss: 1.7252 - val_loss: 1.7714\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.62995\n",
      "Epoch 6/20\n",
      "8484/8484 [==============================] - 627s 74ms/step - loss: 1.6981 - val_loss: 1.6014\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.62995 to 1.60142, saving model to best_weights_lead12.h5\n",
      "Epoch 7/20\n",
      "8484/8484 [==============================] - 626s 74ms/step - loss: 1.6696 - val_loss: 1.6266\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.60142\n",
      "Epoch 8/20\n",
      "8484/8484 [==============================] - 629s 74ms/step - loss: 1.6237 - val_loss: 1.7084\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.60142\n",
      "Epoch 9/20\n",
      "8484/8484 [==============================] - 626s 74ms/step - loss: 1.6093 - val_loss: 1.7024\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.60142\n",
      "Epoch 10/20\n",
      "8484/8484 [==============================] - 624s 74ms/step - loss: 1.5654 - val_loss: 2.0504\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.60142\n",
      "Epoch 11/20\n",
      "8484/8484 [==============================] - 630s 74ms/step - loss: 1.5401 - val_loss: 1.7930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:   6%|███▍                                                      | 1/17 [1:55:32<30:48:36, 6932.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00011: val_loss did not improve from 1.60142\n",
      "******************** counter************* 28\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 618s 73ms/step - loss: 2.5139 - val_loss: 0.9302\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.93016, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 644s 76ms/step - loss: 2.4311 - val_loss: 0.9837\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.93016\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 645s 76ms/step - loss: 2.3786 - val_loss: 1.0212\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.93016\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 646s 76ms/step - loss: 2.3352 - val_loss: 0.8979\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.93016 to 0.89786, saving model to best_weights_lead12.h5\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 641s 76ms/step - loss: 2.2966 - val_loss: 0.9926\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.89786\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 641s 76ms/step - loss: 2.2505 - val_loss: 1.0400\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.89786\n",
      "Epoch 7/20\n",
      "8460/8460 [==============================] - 580s 69ms/step - loss: 2.2182 - val_loss: 0.9912\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.89786\n",
      "Epoch 8/20\n",
      "8460/8460 [==============================] - 570s 67ms/step - loss: 2.1804 - val_loss: 1.2093\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.89786\n",
      "Epoch 9/20\n",
      "8460/8460 [==============================] - 515s 61ms/step - loss: 2.1339 - val_loss: 1.0603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  12%|██████▊                                                   | 2/17 [3:28:00<25:29:28, 6117.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00009: val_loss did not improve from 0.89786\n",
      "******************** counter************* 29\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 543s 64ms/step - loss: 2.2130 - val_loss: 0.4011\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.40109, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 526s 62ms/step - loss: 2.1254 - val_loss: 0.4213\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.40109\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 508s 60ms/step - loss: 2.0770 - val_loss: 0.3667\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.40109 to 0.36669, saving model to best_weights_lead12.h5\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 528s 62ms/step - loss: 2.0261 - val_loss: 0.3398\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36669 to 0.33983, saving model to best_weights_lead12.h5\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 535s 63ms/step - loss: 1.9952 - val_loss: 0.4209\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.33983\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 529s 63ms/step - loss: 1.9756 - val_loss: 0.3690\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.33983\n",
      "Epoch 7/20\n",
      "8460/8460 [==============================] - 511s 60ms/step - loss: 1.9303 - val_loss: 0.4187\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.33983\n",
      "Epoch 8/20\n",
      "8460/8460 [==============================] - 518s 61ms/step - loss: 1.9023 - val_loss: 0.3924\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.33983\n",
      "Epoch 9/20\n",
      "8460/8460 [==============================] - 525s 62ms/step - loss: 1.8602 - val_loss: 0.4362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  18%|██████████▏                                               | 3/17 [4:47:42<21:25:15, 5508.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00009: val_loss did not improve from 0.33983\n",
      "******************** counter************* 30\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 553s 65ms/step - loss: 1.1514 - val_loss: 0.7977\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.79770, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 524s 62ms/step - loss: 1.0965 - val_loss: 0.7525\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.79770 to 0.75246, saving model to best_weights_lead12.h5\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 531s 63ms/step - loss: 1.0712 - val_loss: 0.7955\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.75246\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 540s 64ms/step - loss: 1.0670 - val_loss: 0.7823\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.75246\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 543s 64ms/step - loss: 1.0431 - val_loss: 0.7580\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.75246\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 531s 63ms/step - loss: 1.0151 - val_loss: 0.7485\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.75246 to 0.74850, saving model to best_weights_lead12.h5\n",
      "Epoch 7/20\n",
      "8460/8460 [==============================] - 529s 63ms/step - loss: 0.9965 - val_loss: 0.7393\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.74850 to 0.73930, saving model to best_weights_lead12.h5\n",
      "Epoch 8/20\n",
      "8460/8460 [==============================] - 540s 64ms/step - loss: 0.9823 - val_loss: 0.7795\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.73930\n",
      "Epoch 9/20\n",
      "8460/8460 [==============================] - 542s 64ms/step - loss: 0.9563 - val_loss: 0.7747\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.73930\n",
      "Epoch 10/20\n",
      "8460/8460 [==============================] - 539s 64ms/step - loss: 0.9368 - val_loss: 0.7699\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.73930\n",
      "Epoch 11/20\n",
      "8460/8460 [==============================] - 528s 62ms/step - loss: 0.9326 - val_loss: 0.8351\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.73930\n",
      "Epoch 12/20\n",
      "8460/8460 [==============================] - 537s 64ms/step - loss: 0.9086 - val_loss: 0.8052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  24%|█████████████▋                                            | 4/17 [6:35:58<21:17:55, 5898.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00012: val_loss did not improve from 0.73930\n",
      "******************** counter************* 31\n",
      "Train on 8484 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8484/8484 [==============================] - 558s 66ms/step - loss: 0.9418 - val_loss: 0.8748\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.87477, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8484/8484 [==============================] - 547s 65ms/step - loss: 0.8987 - val_loss: 0.8730\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.87477 to 0.87304, saving model to best_weights_lead12.h5\n",
      "Epoch 3/20\n",
      "8484/8484 [==============================] - 541s 64ms/step - loss: 0.8801 - val_loss: 0.8584\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.87304 to 0.85844, saving model to best_weights_lead12.h5\n",
      "Epoch 4/20\n",
      "8484/8484 [==============================] - 552s 65ms/step - loss: 0.8449 - val_loss: 0.8896\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.85844\n",
      "Epoch 5/20\n",
      "8484/8484 [==============================] - 556s 65ms/step - loss: 0.8461 - val_loss: 0.8654\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.85844\n",
      "Epoch 6/20\n",
      "8484/8484 [==============================] - 557s 66ms/step - loss: 0.8140 - val_loss: 0.8628\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.85844\n",
      "Epoch 7/20\n",
      "8484/8484 [==============================] - 541s 64ms/step - loss: 0.8092 - val_loss: 0.8445\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.85844 to 0.84450, saving model to best_weights_lead12.h5\n",
      "Epoch 8/20\n",
      "8484/8484 [==============================] - 553s 65ms/step - loss: 0.7896 - val_loss: 0.9272\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.84450\n",
      "Epoch 9/20\n",
      "8484/8484 [==============================] - 557s 66ms/step - loss: 0.7723 - val_loss: 0.8732\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.84450\n",
      "Epoch 10/20\n",
      "8484/8484 [==============================] - 559s 66ms/step - loss: 0.7517 - val_loss: 0.8210\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.84450 to 0.82103, saving model to best_weights_lead12.h5\n",
      "Epoch 11/20\n",
      "8484/8484 [==============================] - 535s 63ms/step - loss: 0.7369 - val_loss: 1.0484\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.82103\n",
      "Epoch 12/20\n",
      "8484/8484 [==============================] - 549s 65ms/step - loss: 0.7257 - val_loss: 1.0028\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.82103\n",
      "Epoch 13/20\n",
      "8484/8484 [==============================] - 555s 65ms/step - loss: 0.7179 - val_loss: 1.0942\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.82103\n",
      "Epoch 14/20\n",
      "8484/8484 [==============================] - 556s 66ms/step - loss: 0.7087 - val_loss: 0.9207\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.82103\n",
      "Epoch 15/20\n",
      "8484/8484 [==============================] - 538s 63ms/step - loss: 0.6945 - val_loss: 0.9958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  29%|█████████████████                                         | 5/17 [8:54:35<22:34:04, 6770.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00015: val_loss did not improve from 0.82103\n",
      "******************** counter************* 32\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 570s 67ms/step - loss: 2.6250 - val_loss: 0.4353\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.43530, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 571s 67ms/step - loss: 2.5385 - val_loss: 0.3166\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.43530 to 0.31660, saving model to best_weights_lead12.h5\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 574s 68ms/step - loss: 2.4840 - val_loss: 0.3919\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.31660\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 558s 66ms/step - loss: 2.4332 - val_loss: 0.2972\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.31660 to 0.29721, saving model to best_weights_lead12.h5\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 564s 67ms/step - loss: 2.4074 - val_loss: 0.2731\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.29721 to 0.27308, saving model to best_weights_lead12.h5\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 570s 67ms/step - loss: 2.3650 - val_loss: 0.3159\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.27308\n",
      "Epoch 7/20\n",
      "8460/8460 [==============================] - 576s 68ms/step - loss: 2.3331 - val_loss: 0.2970\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.27308\n",
      "Epoch 8/20\n",
      "8460/8460 [==============================] - 554s 66ms/step - loss: 2.2939 - val_loss: 0.2541\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.27308 to 0.25411, saving model to best_weights_lead12.h5\n",
      "Epoch 9/20\n",
      "8460/8460 [==============================] - 564s 67ms/step - loss: 2.2657 - val_loss: 0.2963\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.25411\n",
      "Epoch 10/20\n",
      "8460/8460 [==============================] - 562s 66ms/step - loss: 2.2295 - val_loss: 0.3151\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.25411\n",
      "Epoch 11/20\n",
      "8460/8460 [==============================] - 568s 67ms/step - loss: 2.2032 - val_loss: 0.4248\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.25411\n",
      "Epoch 12/20\n",
      "8460/8460 [==============================] - 554s 65ms/step - loss: 2.1669 - val_loss: 0.3139\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.25411\n",
      "Epoch 13/20\n",
      "8460/8460 [==============================] - 559s 66ms/step - loss: 2.1336 - val_loss: 0.3241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  35%|████████████████████                                     | 6/17 [10:58:03<21:20:59, 6987.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00013: val_loss did not improve from 0.25411\n",
      "******************** counter************* 33\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 582s 69ms/step - loss: 2.6682 - val_loss: 0.2287\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.22865, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 572s 68ms/step - loss: 2.5980 - val_loss: 0.2355\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.22865\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 560s 66ms/step - loss: 2.5619 - val_loss: 0.3305\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.22865\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 563s 67ms/step - loss: 2.5275 - val_loss: 0.2767\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.22865\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 569s 67ms/step - loss: 2.5000 - val_loss: 0.2335\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.22865\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 577s 68ms/step - loss: 2.4793 - val_loss: 0.2549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  41%|███████████████████████▍                                 | 7/17 [11:56:12<16:13:54, 5843.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: val_loss did not improve from 0.22865\n",
      "******************** counter************* 34\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 578s 68ms/step - loss: 1.7732 - val_loss: 1.5630\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.56300, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 572s 68ms/step - loss: 1.7215 - val_loss: 1.4709\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.56300 to 1.47091, saving model to best_weights_lead12.h5\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 581s 69ms/step - loss: 1.6862 - val_loss: 1.5735\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.47091\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 588s 69ms/step - loss: 1.6840 - val_loss: 1.5499\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.47091\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 575s 68ms/step - loss: 1.6503 - val_loss: 1.5180\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.47091\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 578s 68ms/step - loss: 1.6303 - val_loss: 1.5706\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.47091\n",
      "Epoch 7/20\n",
      "8460/8460 [==============================] - 584s 69ms/step - loss: 1.6146 - val_loss: 1.5202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  47%|██████████████████████████▊                              | 8/17 [13:04:54<13:14:20, 5295.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00007: val_loss did not improve from 1.47091\n",
      "******************** counter************* 35\n",
      "Train on 8484 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8484/8484 [==============================] - 604s 71ms/step - loss: 1.9852 - val_loss: 1.0915\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.09147, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8484/8484 [==============================] - 578s 68ms/step - loss: 1.9323 - val_loss: 1.0231\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.09147 to 1.02314, saving model to best_weights_lead12.h5\n",
      "Epoch 3/20\n",
      "8484/8484 [==============================] - 577s 68ms/step - loss: 1.9031 - val_loss: 1.0282\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.02314\n",
      "Epoch 4/20\n",
      "8484/8484 [==============================] - 588s 69ms/step - loss: 1.8824 - val_loss: 0.9923\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.02314 to 0.99226, saving model to best_weights_lead12.h5\n",
      "Epoch 5/20\n",
      "8484/8484 [==============================] - 589s 69ms/step - loss: 1.8453 - val_loss: 1.1783\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.99226\n",
      "Epoch 6/20\n",
      "8484/8484 [==============================] - 581s 68ms/step - loss: 1.8214 - val_loss: 1.0640\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.99226\n",
      "Epoch 7/20\n",
      "8484/8484 [==============================] - 577s 68ms/step - loss: 1.7998 - val_loss: 1.1860\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.99226\n",
      "Epoch 8/20\n",
      "8484/8484 [==============================] - 587s 69ms/step - loss: 1.7798 - val_loss: 1.0585\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.99226\n",
      "Epoch 9/20\n",
      "8484/8484 [==============================] - 591s 70ms/step - loss: 1.7456 - val_loss: 1.1219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  53%|██████████████████████████████▏                          | 9/17 [14:33:55<11:47:58, 5309.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00009: val_loss did not improve from 0.99226\n",
      "******************** counter************* 36\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 614s 73ms/step - loss: 1.9868 - val_loss: 0.8658\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.86584, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 610s 72ms/step - loss: 1.9021 - val_loss: 0.7982\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.86584 to 0.79822, saving model to best_weights_lead12.h5\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 603s 71ms/step - loss: 1.8733 - val_loss: 0.8364\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.79822\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 600s 71ms/step - loss: 1.8233 - val_loss: 0.7473\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.79822 to 0.74733, saving model to best_weights_lead12.h5\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 584s 69ms/step - loss: 1.8089 - val_loss: 0.8519\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.74733\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 596s 71ms/step - loss: 1.7868 - val_loss: 0.7742\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.74733\n",
      "Epoch 7/20\n",
      "8460/8460 [==============================] - 606s 72ms/step - loss: 1.7562 - val_loss: 0.8280\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.74733\n",
      "Epoch 8/20\n",
      "8460/8460 [==============================] - 611s 72ms/step - loss: 1.7282 - val_loss: 0.8142\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.74733\n",
      "Epoch 9/20\n",
      "8460/8460 [==============================] - 594s 70ms/step - loss: 1.7084 - val_loss: 0.9281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  59%|████████████████████████████████▉                       | 10/17 [16:05:27<10:26:02, 5366.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00009: val_loss did not improve from 0.74733\n",
      "******************** counter************* 37\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 630s 74ms/step - loss: 1.9638 - val_loss: 0.6262\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.62616, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 621s 73ms/step - loss: 1.8909 - val_loss: 0.6653\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.62616\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 623s 74ms/step - loss: 1.8754 - val_loss: 0.6112\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62616 to 0.61116, saving model to best_weights_lead12.h5\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 610s 72ms/step - loss: 1.8365 - val_loss: 0.6138\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.61116\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 621s 73ms/step - loss: 1.8020 - val_loss: 0.6940\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.61116\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 625s 74ms/step - loss: 1.7809 - val_loss: 0.7586\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61116\n",
      "Epoch 7/20\n",
      "8460/8460 [==============================] - 618s 73ms/step - loss: 1.7550 - val_loss: 0.6333\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61116\n",
      "Epoch 8/20\n",
      "8460/8460 [==============================] - 615s 73ms/step - loss: 1.7363 - val_loss: 0.6561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  65%|████████████████████████████████████▉                    | 11/17 [17:29:26<8:46:34, 5265.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61116\n",
      "******************** counter************* 38\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 634s 75ms/step - loss: 1.1750 - val_loss: 1.2003\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.20027, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 613s 72ms/step - loss: 1.1135 - val_loss: 1.2219\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.20027\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 601s 71ms/step - loss: 1.0903 - val_loss: 1.2811\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.20027\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 606s 72ms/step - loss: 1.0681 - val_loss: 1.1160\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.20027 to 1.11605, saving model to best_weights_lead12.h5\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 611s 72ms/step - loss: 1.0404 - val_loss: 1.2102\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.11605\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 614s 73ms/step - loss: 1.0366 - val_loss: 1.0730\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.11605 to 1.07295, saving model to best_weights_lead12.h5\n",
      "Epoch 7/20\n",
      "8460/8460 [==============================] - 603s 71ms/step - loss: 1.0278 - val_loss: 1.1241\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.07295\n",
      "Epoch 8/20\n",
      "8460/8460 [==============================] - 614s 73ms/step - loss: 1.0098 - val_loss: 1.0895\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.07295\n",
      "Epoch 9/20\n",
      "8460/8460 [==============================] - 627s 74ms/step - loss: 0.9909 - val_loss: 1.0814\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.07295\n",
      "Epoch 10/20\n",
      "8460/8460 [==============================] - 658s 78ms/step - loss: 0.9899 - val_loss: 1.2031\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.07295\n",
      "Epoch 11/20\n",
      "8460/8460 [==============================] - 781s 92ms/step - loss: 0.9851 - val_loss: 1.1193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  71%|████████████████████████████████████████▏                | 12/17 [19:26:55<8:04:01, 5808.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00011: val_loss did not improve from 1.07295\n",
      "******************** counter************* 39\n",
      "Train on 8484 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8484/8484 [==============================] - 791s 93ms/step - loss: 1.2952 - val_loss: 1.2944\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.29443, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8484/8484 [==============================] - 794s 94ms/step - loss: 1.2638 - val_loss: 1.1871\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.29443 to 1.18713, saving model to best_weights_lead12.h5\n",
      "Epoch 3/20\n",
      "8484/8484 [==============================] - 789s 93ms/step - loss: 1.2472 - val_loss: 1.3566\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.18713\n",
      "Epoch 4/20\n",
      "8484/8484 [==============================] - 788s 93ms/step - loss: 1.2212 - val_loss: 1.2500\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.18713\n",
      "Epoch 5/20\n",
      "8484/8484 [==============================] - 788s 93ms/step - loss: 1.1880 - val_loss: 1.2693\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.18713\n",
      "Epoch 6/20\n",
      "8484/8484 [==============================] - 790s 93ms/step - loss: 1.1734 - val_loss: 1.2606\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.18713\n",
      "Epoch 7/20\n",
      "8484/8484 [==============================] - 782s 92ms/step - loss: 1.1520 - val_loss: 1.1992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  76%|███████████████████████████████████████████▌             | 13/17 [21:00:10<6:22:54, 5743.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00007: val_loss did not improve from 1.18713\n",
      "******************** counter************* 40\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 785s 93ms/step - loss: 1.2424 - val_loss: 0.5900\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.59002, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 791s 93ms/step - loss: 1.2050 - val_loss: 0.5938\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.59002\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 790s 93ms/step - loss: 1.1803 - val_loss: 0.6200\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.59002\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 782s 92ms/step - loss: 1.1641 - val_loss: 0.5979\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.59002\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 797s 94ms/step - loss: 1.1564 - val_loss: 0.6447\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.59002\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 798s 94ms/step - loss: 1.1514 - val_loss: 0.6086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  82%|██████████████████████████████████████████████▉          | 14/17 [22:20:31<4:33:14, 5464.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: val_loss did not improve from 0.59002\n",
      "******************** counter************* 41\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 820s 97ms/step - loss: 2.9249 - val_loss: 0.6469\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64687, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 808s 96ms/step - loss: 2.8748 - val_loss: 0.6307\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64687 to 0.63070, saving model to best_weights_lead12.h5\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 816s 96ms/step - loss: 2.8533 - val_loss: 0.6580\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.63070\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 820s 97ms/step - loss: 2.8335 - val_loss: 0.6344\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.63070\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 823s 97ms/step - loss: 2.8226 - val_loss: 0.6747\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.63070\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 826s 98ms/step - loss: 2.8018 - val_loss: 0.6306\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.63070 to 0.63060, saving model to best_weights_lead12.h5\n",
      "Epoch 7/20\n",
      "8460/8460 [==============================] - 824s 97ms/step - loss: 2.7798 - val_loss: 0.7193\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.63060\n",
      "Epoch 8/20\n",
      "8460/8460 [==============================] - 826s 98ms/step - loss: 2.7670 - val_loss: 0.6756\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.63060\n",
      "Epoch 9/20\n",
      "8460/8460 [==============================] - 820s 97ms/step - loss: 2.7295 - val_loss: 0.6844\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.63060\n",
      "Epoch 10/20\n",
      "8460/8460 [==============================] - 824s 97ms/step - loss: 2.7175 - val_loss: 0.6257\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.63060 to 0.62570, saving model to best_weights_lead12.h5\n",
      "Epoch 11/20\n",
      "8460/8460 [==============================] - 824s 97ms/step - loss: 2.7168 - val_loss: 0.7351\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.62570\n",
      "Epoch 12/20\n",
      "8460/8460 [==============================] - 816s 96ms/step - loss: 2.6964 - val_loss: 0.7128\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.62570\n",
      "Epoch 13/20\n",
      "8460/8460 [==============================] - 823s 97ms/step - loss: 2.6791 - val_loss: 0.7050\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.62570\n",
      "Epoch 14/20\n",
      "8460/8460 [==============================] - 822s 97ms/step - loss: 2.6847 - val_loss: 0.7285\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.62570\n",
      "Epoch 15/20\n",
      "8460/8460 [==============================] - 827s 98ms/step - loss: 2.6700 - val_loss: 0.7494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  88%|██████████████████████████████████████████████████▎      | 15/17 [25:47:10<4:11:50, 7555.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00015: val_loss did not improve from 0.62570\n",
      "******************** counter************* 42\n",
      "Train on 8460 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8460/8460 [==============================] - 832s 98ms/step - loss: 1.7335 - val_loss: 2.3870\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.38698, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8460/8460 [==============================] - 838s 99ms/step - loss: 1.6876 - val_loss: 2.4986\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.38698\n",
      "Epoch 3/20\n",
      "8460/8460 [==============================] - 837s 99ms/step - loss: 1.6591 - val_loss: 2.3719\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.38698 to 2.37185, saving model to best_weights_lead12.h5\n",
      "Epoch 4/20\n",
      "8460/8460 [==============================] - 837s 99ms/step - loss: 1.6474 - val_loss: 2.4586\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.37185\n",
      "Epoch 5/20\n",
      "8460/8460 [==============================] - 836s 99ms/step - loss: 1.6252 - val_loss: 2.4354\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.37185\n",
      "Epoch 6/20\n",
      "8460/8460 [==============================] - 834s 99ms/step - loss: 1.6230 - val_loss: 2.4187\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.37185\n",
      "Epoch 7/20\n",
      "8460/8460 [==============================] - 832s 98ms/step - loss: 1.5968 - val_loss: 2.4742\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.37185\n",
      "Epoch 8/20\n",
      "8460/8460 [==============================] - 837s 99ms/step - loss: 1.5860 - val_loss: 2.5012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  94%|█████████████████████████████████████████████████████▋   | 16/17 [27:39:57<2:01:57, 7317.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00008: val_loss did not improve from 2.37185\n",
      "******************** counter************* 43\n",
      "Train on 8484 samples, validate on 276 samples\n",
      "Epoch 1/20\n",
      "8484/8484 [==============================] - 853s 101ms/step - loss: 1.3450 - val_loss: 1.0542\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.05418, saving model to best_weights_lead12.h5\n",
      "Epoch 2/20\n",
      "8484/8484 [==============================] - 857s 101ms/step - loss: 1.2957 - val_loss: 1.2057\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.05418\n",
      "Epoch 3/20\n",
      "8484/8484 [==============================] - 861s 101ms/step - loss: 1.2741 - val_loss: 1.0636\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.05418\n",
      "Epoch 4/20\n",
      "8484/8484 [==============================] - 861s 102ms/step - loss: 1.2827 - val_loss: 1.3855\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.05418\n",
      "Epoch 5/20\n",
      "8484/8484 [==============================] - 851s 100ms/step - loss: 1.2624 - val_loss: 1.0724\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.05418\n",
      "Epoch 6/20\n",
      "8484/8484 [==============================] - 856s 101ms/step - loss: 1.2814 - val_loss: 1.1498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|███████████████████████████████████████████████████████████| 17/17 [29:07:02<00:00, 6166.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: val_loss did not improve from 1.05418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "num_epochs = 8\n",
    "lead=12\n",
    "count=27\n",
    "for loop in tqdm(fileList_train, desc=\"Processing files\"):\n",
    "    print('******************** counter*************',count)\n",
    "    File=nc.Dataset(loop)\n",
    "    Z=np.asarray(File['t'])\n",
    "    \n",
    "# # Create a mask to filter out samples with NaN values\n",
    "#     mask = np.all(~np.isnan(Z), axis=(1, 2))\n",
    "#     Z = Z[mask]\n",
    "    \n",
    "    trainN=np.size(Z,0)-300\n",
    "    Z=(Z-M)/sdev\n",
    "    \n",
    "    \n",
    "    x_train=Z[0:trainN,:,:]\n",
    "    x_train=x_train.reshape([np.size(x_train,0),32,64,1])\n",
    "    y_train=Z[lead:trainN+lead,:,:]\n",
    "    y_train=y_train.reshape([np.size(y_train,0),32,64,1])\n",
    "    \n",
    "    x_val= Z[trainN+lead:np.size(Z,0)-lead,:,:]\n",
    "    x_val=x_val.reshape([np.size(x_val,0),32,64,1])\n",
    "    \n",
    "    y_val= Z[trainN+lead*2:np.size(Z,0),:,:]\n",
    "    y_val=y_val.reshape([np.size(y_val,0),32,64,1])\n",
    "\n",
    "    if (count>0):\n",
    "        \n",
    "        model = stn()\n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "        # Modify the HDF5 file before loading weights\n",
    "#         modify_h5_file('best_weights_lead12.h5')\n",
    "\n",
    "        model.load_weights('best_weights_lead12.h5')\n",
    "        hist = model.fit(x_train, y_train,\n",
    "                       batch_size = batch_size,\n",
    "             verbose=1,\n",
    "             epochs = 20,\n",
    "             validation_data=(x_val,y_val),shuffle=True,\n",
    "             callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                        min_delta=0,\n",
    "                                        patience=5, # just to make sure we use a lot of patience before stopping\n",
    "                                        verbose=0, mode='auto'),\n",
    "                       keras.callbacks.ModelCheckpoint('best_weights_lead'+str(lead)+'.h5', monitor='val_loss',\n",
    "                                                    verbose=1, save_best_only=True,\n",
    "                                                    save_weights_only=True, mode='auto', period=1),history]\n",
    "             )\n",
    "        losses.extend(hist.history['loss'])\n",
    "        val_losses.extend(hist.history['val_loss'])\n",
    "\n",
    "    else:\n",
    "        hist = model.fit(x_train, y_train,\n",
    "                       batch_size = batch_size,\n",
    "             verbose=1,\n",
    "             epochs = 20,\n",
    "             validation_data=(x_val,y_val),shuffle=True,\n",
    "             callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                        min_delta=0,\n",
    "                                        patience=5, # just to make sure we use a lot of patience before stopping\n",
    "                                        verbose=0, mode='auto'),\n",
    "                       keras.callbacks.ModelCheckpoint('best_weights_lead'+str(lead)+'.h5', monitor='val_loss',\n",
    "                                                    verbose=1, save_best_only=True,\n",
    "                                                    save_weights_only=True, mode='auto', period=1),history]\n",
    "             )\n",
    "        losses.extend(hist.history['loss'])\n",
    "        val_losses.extend(hist.history['val_loss'])\n",
    "\n",
    "\n",
    "    count=count+1"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
