{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73fc577f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T15:03:26.551339Z",
     "start_time": "2023-09-02T15:03:26.544503Z"
    }
   },
   "outputs": [],
   "source": [
    "# Basic setting for Jupyter_notebook to import utils\n",
    "import os\n",
    "import sys\n",
    "\n",
    "notebook_path = os.path.abspath(\"\")\n",
    "project_root = os.path.abspath(os.path.join(notebook_path, \"../../\"))\n",
    "\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bb8b09a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T15:03:31.965124Z",
     "start_time": "2023-09-02T15:03:27.213512Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from utils import folder_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923bc26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "\n",
    "@author: Ashesh Chattopadhyay\n",
    "This is a hybird SPEnKF implementation with U-STNx as the backgroud forecast model.\n",
    "\n",
    "More details in paper: https://gmd.copernicus.org/preprints/gmd-2021-71/\n",
    "\n",
    "The github repository contains an jupyter notebook to train the U-STNx model with different values of \"x\"\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5c0411",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This .mat file has been generated from the ERA5 lat-lon data ####\n",
    "# file=sio.loadmat('ERA_grid.mat')\n",
    "# lat=file['lat']\n",
    "# lon=file['lon']\n",
    "\n",
    "lat = np.linspace(50.0, 57.75, 32)  # latitude\n",
    "lon = np.linspace(-6.0, 1.875, 64)  # longitude\n",
    "# 3.Define the grid\n",
    "# g_lon = np.linspace(-6.0, 1.875, 64)  # longitude\n",
    "# g_lat = np.linspace(50.0, 57.75, 32)  # latitude\n",
    "# # gridx, gridy = np.meshgrid(gridx, gridy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98e55847",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T15:07:03.268132Z",
     "start_time": "2023-09-02T15:07:03.263785Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input setting\n",
    "# Example usage\n",
    "country = \"GB\"\n",
    "data_folder = \"data\"\n",
    "data_read_category = \"da_test_data\"\n",
    "data_test_category = \"test_data\"\n",
    "data_save_category = \"assimilated_data\"\n",
    "output_folder = \"2022_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e0f6cd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T15:07:27.508172Z",
     "start_time": "2023-09-02T15:07:27.503818Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_era5_list(country, data_folder, data_category, output_folder):\n",
    "    input_folder_path = folder_utils.find_folder(\n",
    "        country, data_folder, data_category, output_folder\n",
    "    )\n",
    "    nc_files = [\n",
    "        f for f in os.listdir(input_folder_path) if f.endswith(\".nc\")\n",
    "    ]\n",
    "    return [\n",
    "        os.path.join(input_folder_path, nc_file) for nc_file in nc_files\n",
    "    ]  # list for era5 nc files path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08f7669f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T15:09:02.677473Z",
     "start_time": "2023-09-02T15:09:02.667470Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/ww721/JupyterNotebookPath/IRP_20220602/irp_ww721_bakcup/data/da_test_data/2022_data/GB_2022_data/asos_kridge_2022.nc',\n",
       " '/Users/ww721/JupyterNotebookPath/IRP_20220602/irp_ww721_bakcup/data/da_test_data/2022_data/GB_2022_data/era5_pressure_level_2022_regrid_filter_850.nc']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########## This is the testing set #######\n",
    "\n",
    "fileList_test=get_era5_list(country,data_folder,data_read_category,output_folder)\n",
    "fileList_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6570644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Ensure same normalization coefficient as trainig #######\n",
    "M0 = 273.77817\n",
    "sdev = 2.5819736"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7689e535",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### True data (noise free) for twin DA experiments ##########\n",
    "## here can be modified \n",
    "\n",
    "F=nc.Dataset(fileList_test[1])\n",
    "Z=np.asarray(F['t'])\n",
    "TRUTH=Z\n",
    "\n",
    "### Meshgrid for plotting ###\n",
    "[qx,qy]=np.meshgrid(lon,lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fa4f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Add noise to the truth to mimic observations####\n",
    "#### Value 1 is 1*\\sigma_Z. See more in paper #####\n",
    "Z_rs = np.reshape(Z,[np.size(Z,0), int(np.size(Z,1)*np.size(Z,2))])\n",
    "TRUTH = Z_rs\n",
    "Z_rs = (Z_rs-M)/sdev\n",
    "TRUTH = (TRUTH-M)/sdev\n",
    "noise=1 # modify here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e9e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1,np.size(Z_rs,0)):\n",
    "    Z_rs[k-1,:]=Z_rs[k-1,:]+np.random.normal(0, noise, 2048)\n",
    "print('length of initial condition',len(Z_rs[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a66bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SPNEKF implementation following Tyrus Berry's implementation ######\n",
    "\n",
    "def ENKF(x, n, P ,Q, R, obs, model, u_ensemble):\n",
    "    obs=np.reshape(obs,[n,1]) \n",
    "    x=np.reshape(x,[n,1])\n",
    "    [U,S,V]=np.linalg.svd(P)\n",
    "    D=np.zeros([n,n])\n",
    "    np.fill_diagonal(D,S)\n",
    "    sqrtP=np.dot(np.dot(U,np.sqrt(D)),U)\n",
    "    ens=np.zeros([n,2*n])\n",
    "    ens[:,0:n]=np.tile(x,(1,n)) + sqrtP\n",
    "    ens[:,n:]=np.tile(x,(1,n)) - sqrtP\n",
    "    ## forecasting step,dummy model\n",
    "\n",
    "    for k in range(0, np.size(ens,1)):\n",
    "\n",
    "       u =  model.predict(np.reshape(ens[:,k],[1, 32, 64, 1]))\n",
    "\n",
    "       u_ensemble[:,k]=np.reshape(u,(32*64,))\n",
    "\n",
    "\n",
    "\n",
    "    ############################\n",
    "    x_prior = np.reshape(np.mean(u_ensemble,1),[n,1])\n",
    "    print('shape pf x_prior',np.shape(x_prior))\n",
    "    print('shape pf obs',np.shape(obs))\n",
    "    cf_ens = ens - np.tile(x_prior,(1,2*n))\n",
    "    P_prior = np.dot(cf_ens,np.transpose(cf_ens))/(2*n - 1)+Q\n",
    "    h_ens = ens\n",
    "    y_prior=np.reshape(np.mean(h_ens,1),[n,1])\n",
    "    ch_ens = h_ens - np.tile(y_prior,(1,2*n))\n",
    "    print('shape pf y_prior',np.shape(y_prior))\n",
    "    P_y = np.dot(ch_ens, np.transpose(ch_ens))/(2*n-1) + R\n",
    "    P_xy = np.dot(cf_ens, np.transpose(ch_ens)) /(2*n-1)\n",
    "    K = np.dot(P_xy,np.linalg.inv(P_y))\n",
    "    P = P_prior - np.dot(np.dot(K,P_y),np.transpose(K))\n",
    "    x = x_prior + np.dot(K,(obs-y_prior))\n",
    "\n",
    "    return x, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd9100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import keras.backend as K\n",
    "#from data_manager import ClutteredMNIST\n",
    "#from visualizer import plot_mnist_sample\n",
    "#from visualizer import print_evaluation\n",
    "#from visualizer import plot_mnist_grid\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "from keras.layers import Input, Convolution2D, Convolution1D, MaxPooling2D, Dense, Dropout, \\\n",
    "                          Flatten, concatenate, Activation, Reshape, \\\n",
    "                          UpSampling2D,ZeroPadding2D\n",
    "import keras\n",
    "from keras.callbacks import History\n",
    "history = History()\n",
    "\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Cropping2D, Concatenate, ZeroPadding2D\n",
    "from keras.models import load_model\n",
    "\n",
    "__version__ = 0.1\n",
    "\n",
    "\n",
    "#### This is the circular convolution function. With/Without doesn't make much difference. If training is done with CConv2D then replace Convolution2D with CCvonv2D else leave it like this  #####\n",
    "def CConv2D(filters, kernel_size, strides=(1, 1), activation='linear', padding='valid', kernel_initializer='glorot_uniform', kernel_regularizer=None):\n",
    "    def CConv2D_inner(x):\n",
    "        # padding (see https://www.tensorflow.org/api_guides/python/nn#Convolution)\n",
    "        in_height = int(x.get_shape()[1])\n",
    "        in_width = int(x.get_shape()[2])\n",
    "\n",
    "        if (in_height % strides[0] == 0):\n",
    "            pad_along_height = max(kernel_size[0] - strides[0], 0)\n",
    "        else:\n",
    "            pad_along_height = max(\n",
    "                kernel_size[0] - (in_height % strides[0]), 0)\n",
    "        if (in_width % strides[1] == 0):\n",
    "            pad_along_width = max(kernel_size[1] - strides[1], 0)\n",
    "        else:\n",
    "            pad_along_width = max(kernel_size[1] - (in_width % strides[1]), 0)\n",
    "\n",
    "        pad_top = pad_along_height // 2\n",
    "        pad_bottom = pad_along_height - pad_top\n",
    "        pad_left = pad_along_width // 2\n",
    "        pad_right = pad_along_width - pad_left\n",
    "\n",
    "        # left and right side for padding\n",
    "        pad_left = Cropping2D(cropping=((0, 0), (in_width-pad_left, 0)))(x)\n",
    "        pad_right = Cropping2D(cropping=((0, 0), (0, in_width-pad_right)))(x)\n",
    "\n",
    "        # add padding to incoming image\n",
    "        conc = Concatenate(axis=2)([pad_left, x, pad_right])\n",
    "\n",
    "        # top/bottom padding options\n",
    "        if padding == 'same':\n",
    "            conc = ZeroPadding2D(padding={'top_pad': pad_top,\n",
    "                                          'bottom_pad': pad_bottom})(conc)\n",
    "        elif padding == 'valid':\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception('Padding \"{}\" does not exist!'.format(padding))\n",
    "\n",
    "        # perform the circular convolution\n",
    "        cconv2d = Conv2D(filters=filters, kernel_size=kernel_size,\n",
    "                         strides=strides, activation=activation,\n",
    "                         padding='valid',\n",
    "                         kernel_initializer=kernel_initializer,\n",
    "                         kernel_regularizer=kernel_regularizer)(conc)\n",
    "\n",
    "        # return circular convolution layer\n",
    "        return cconv2d\n",
    "    return CConv2D_inner\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Dense\n",
    "\n",
    "from utils import get_initial_weights\n",
    "from layers import BilinearInterpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e43b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Load model. DO not train. #####\n",
    "def stn(input_shape=(32, 64, 1), sampling_size=(8, 16), num_classes=10):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    conv1 = Convolution2D(32, 5, 5, activation='relu', border_mode='same')(inputs)\n",
    "    conv1 = Convolution2D(32, 5, 5, activation='relu', border_mode='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Convolution2D(32, 5, 5, activation='relu', border_mode='same')(pool1)\n",
    "    conv2 = Convolution2D(32, 5, 5, activation='relu', border_mode='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Convolution2D(32, 5, 5, activation='relu', border_mode='same')(pool2)\n",
    "    conv3 = Convolution2D(32, 5, 5, activation='relu', border_mode='same')(conv3)\n",
    "\n",
    "\n",
    "    conv5 = Convolution2D(32, 5, 5, activation='relu', border_mode='same')(conv3)\n",
    "    conv5 = Convolution2D(32, 5, 5, activation='relu', border_mode='same')(conv5)\n",
    "    \n",
    "    locnet = Flatten()(conv5)\n",
    "    locnet = Dense(500)(locnet)\n",
    "    locnet = Activation('relu')(locnet)\n",
    "    locnet = Dense(200)(locnet)\n",
    "    locnet = Activation('relu')(locnet)\n",
    "    locnet = Dense(100)(locnet)\n",
    "    locnet = Activation('relu')(locnet)\n",
    "    locnet = Dense(50)(locnet)\n",
    "    locnet = Activation('relu')(locnet)\n",
    "    weights = get_initial_weights(50)\n",
    "    locnet = Dense(6, weights=weights)(locnet)\n",
    "    x = BilinearInterpolation(sampling_size)([inputs, locnet])\n",
    "\n",
    "\n",
    "    up6 = keras.layers.Concatenate(axis=-1)([Convolution2D(32, 2, 2,activation='relu', border_mode='same')(UpSampling2D(size=(2, 2))(x)), conv2])\n",
    "    conv6 = Convolution2D(32, 5, 5, activation='relu', border_mode='same')(up6)\n",
    "    conv6 = Convolution2D(32, 5, 5, activation='relu', border_mode='same')(conv6)\n",
    "\n",
    "    up7 = keras.layers.Concatenate(axis=-1)([Convolution2D(32, 2, 2,activation='relu', border_mode='same')(UpSampling2D(size=(2, 2))(conv6)), conv1])\n",
    "    conv7 = Convolution2D(32, 5, 5, activation='relu', border_mode='same')(up7)\n",
    "    conv7 = Convolution2D(32, 5, 5, activation='relu', border_mode='same')(conv7)\n",
    "\n",
    "\n",
    "\n",
    "    conv10 = Convolution2D(1, 5, 5, activation='linear',border_mode='same')(conv7)\n",
    "\n",
    "    model = Model(input=inputs, output=conv10)\n",
    "\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a3ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = stn()\n",
    "model.load_weights('best_weights_lead1.h5') \n",
    "### This code performs DA at every 24 hrs with a model that is forecasting every hour. So lead will always be 1 ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef0122",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Start Data Assimilation Process #########################################\n",
    "\n",
    "time = 1200\n",
    "n=int(32*64)\n",
    "P=np.eye(n,n)\n",
    "\n",
    "Q=0.03*np.eye(n,n)\n",
    "\n",
    "R=0.0001\n",
    "\n",
    "u_ensemble=np.zeros([32*64,2*32*64])\n",
    "\n",
    "pred=np.zeros([time,32,64,1])\n",
    "\n",
    "\n",
    "dt=24\n",
    "count=0\n",
    "for t in range(0, time, dt):\n",
    "    \n",
    "    for kk in range(0,dt-1):\n",
    "        if (kk==0):   \n",
    "          u=Z_rs[t+kk,:].reshape([1, 32, 64, 1 ])\n",
    "          u=model.predict(u.reshape([1,32,64,1]))\n",
    "        else :\n",
    "      \n",
    "          u=model.predict(u)\n",
    "        \n",
    "        pred[count,:,:,0]=np.reshape(u,[32,64])\n",
    "        count=count+1\n",
    "    x=u   \n",
    "    x, P = ENKF(x, 2048, P, Q, R, Z_rs[t+dt,:], model,u_ensemble)\n",
    "   \n",
    "    print('output shape of ENKF', np.shape(x))\n",
    "    \n",
    "    pred[count, :, :, 0] = np.reshape(x, [32, 64])\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "sio.savemat(\n",
    "    \"DA_every24HR_lead1200_everytime_noise_\" + str(noise) + \".mat\",\n",
    "    dict(\n",
    "        [\n",
    "            (\"prediction\", pred),\n",
    "            (\"truth\", np.reshape(TRUTH, [np.size(Z_rs, 0), 32, 64, 1])),\n",
    "            (\"noisy_obs\", np.reshape(Z_rs, [np.size(Z_rs, 0), 32, 64, 1])),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Done writing file\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
